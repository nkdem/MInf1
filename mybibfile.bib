@misc{WHO2024deafness,
  author = "World Health Organization",
  url = "https://www.who.int/news-room/fact-sheets/detail/deafness-and-hearing-loss",
  title = "Deafness and hearing loss",
  year = {2024}
}
@article{Kochkin2010MarkeTrak8,
  author = {Kochkin, Sergei},
  journal = {The Hearing Journal},
  number = {1},
  title = {MarkeTrak {VIII}: Consumer satisfaction with hearing aids is slowly increasing},
  doi = {10.1097/01.HJ.0000366912.40173.76},
  volume = {63},
  year = {2010}
}

@inproceedings{Huwel2020HearDS,
  author = {Hüwel, Andreas and Adiloğlu, Kamil and Bach, Jörg-Hendrik},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  number = {},
  title = {Hearing aid Research Data Set for Acoustic Environment Recognition},
  doi = {10.1109/ICASSP40776.2020.9053611},
  volume = {},
  year = {2020}
}

@article{Korhonen2021WindNoise,
  author = {Korhonen, Petri},
  journal = {Seminars in Hearing},
  number = {3},
  title = {Wind {{Noise Management}} in {{Hearing Aids}}},
  volume = {42},
  year = {2021},
  doi = {10.1055/s-0041-1735133},
}

@article{Stone2002Delays,
  author = {Stone, Michael A. and Moore, Brian C. J.},
  journal = {Ear and Hearing},
  number = {4},
  title = {Tolerable {Hearing Aid Delays}. {II}. Estimation of Limits Imposed During Speech Production},
  volume = {23},
  year = {2002},
  doi = {10.1097/00003446-200208000-00008},
}

@article{Gomez2021MIPS,
  author = {García-Gómez, Joaquín and Gil-Pita, Roberto and Aguilar-Ortega, Miguel and Utrilla-Manso, Manuel and Rosa-Zurera, Manuel and Mohino-Herranz, Inma},
  journal = {Applied Acoustics},
  title = {Linear Detector and Neural Networks in Cascade for Voice Activity Detection in Hearing Aids},
  volume = {175},
  year = {2021},
  doi = {10.1016/j.apacoust.2020.107832}
}
@article{Furness2015HairCell,
  title={Molecular basis of hair cell loss},
  author={Furness, David N.},
  journal={Cell Tissue Research},
  volume={361},
  pages={387--399},
  year={2015},
  publisher={Springer Berlin Heidelberg},
  doi={10.1007/s00441-015-2113-z}
}
@book{
  Wayland2018Phonetics, 
  place={Cambridge}, 
  title={Phonetics: A Practical Introduction}, 
  publisher={Cambridge University Press}, 
  author={Wayland, Ratree}, 
  year={2018},
} 
@article{Nieman2020HearingLoss,
title = {Hearing Loss},
author = {Nieman, Carrie L. and Oh, Esther S.},
journal = {Annals of Internal Medicine},
volume = {173},
number = {11},
pages = {ITC81-ITC96},
year = {2020},
doi = {10.7326/AITC202012010},
url = {https://www.acpjournals.org/doi/abs/10.7326/AITC202012010}
}
@INPROCEEDINGS{Giannoulis2013ASA,
  author={Giannoulis, Dimitrios and Stowell, Dan and Benetos, Emmanouil and Rossignol, Mathias and Lagrange, Mathieu and Plumbley, Mark D.},
  booktitle={21st European Signal Processing Conference (EUSIPCO 2013)}, 
  title={A database and challenge for acoustic scene classification and event detection}, 
  year={2013},
  volume={},
  number={},
  pages={1-5},
}
@INPROCEEDINGS{Pal2013BlindSourceSeparation,
  author={Pal, Madhab and Roy, Rajib and Basu, Joyanta and Bepari, Milton S.},
  booktitle={2013 International Conference Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE)}, 
  title={Blind source separation: A review and analysis}, 
  year={2013},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICSDA.2013.6709849},
  }

@techreport{Hasemann2024PhonakSphere,
  author = {Hasemann Henning, Krylova Alena},
  institution = {Phonak},
  title = {Revolutionary Speech Understanding with Spheric Speech Clarity},
  year = {2024},
  url = {https://www.phonak.com/content/dam/phonak/en/evidence-library/white-paper/technical-paper/PH\_Insight\_SphericSpeechClarity\_210x297\_EN\_028-2684-02\_V1.00.pdf},
}


@article{levitt_historical_2007,
	title = {A {Historical} {Perspective} on {Digital} {Hearing} {Aids}: {How} {Digital} {Technology} {Has} {Changed} {Modern} {Hearing} {Aids}},
	volume = {11},
	shorttitle = {A {Historical} {Perspective} on {Digital} {Hearing} {Aids}},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC4111501/},
	doi = {10.1177/1084713806298000},
	abstract = {This article provides the author's perspective on the development of digital hearing aids and how digital signal processing approaches have led to changes in hearing aid design. Major landmarks in the evolution of digital technology are identified, ...},
	language = {en},
	number = {1},
	urldate = {2025-03-10},
	journal = {Trends in Amplification},
	author = {Levitt, Harry},
	month = mar,
	year = {2007},
	pmid = {17301334},
	pages = {7},
	file = {Full Text PDF:/Users/nkdem/Zotero/storage/P4YZXJC6/Levitt - 2007 - A Historical Perspective on Digital Hearing Aids How Digital Technology Has Changed Modern Hearing.pdf:application/pdf},
}


@misc{magicss_hearing_2020,
	title = {Hearing {Aid} {History}: {From} {Ear} {Trumpets} to {Digital} {Technology}},
	shorttitle = {Hearing {Aid} {History}},
	url = {https://www.embs.org/pulse/articles/hearing-aid-history-from-ear-trumpets-to-digital-technology/},
	abstract = {Author(s): Max ValentinuzziIt is said that time marches on, and one thing is certain: Hearing loss marches right along with it. The recorded history of hearing loss goes back hundreds of years, and attempts to correct hearing loss have been in existence since the very first person to cup a hand behind one ear. Continue Reading Hearing Aid History: From Ear Trumpets to Digital Technology},
	language = {en-US},
	urldate = {2025-03-10},
	journal = {IEEE Pulse},
	author = {magicss},
	month = oct,
	year = {2020},
	file = {Snapshot:/Users/nkdem/Zotero/storage/4ZHJRUBB/hearing-aid-history-from-ear-trumpets-to-digital-technology.html:text/html},
}

@inproceedings{DCASE2017challenge,
    Author = "Mesaros, A. and Heittola, T. and Diment, A. and Elizalde, B. and Shah, A. and Vincent, E. and Raj, B. and Virtanen, T.",
    title = "{DCASE} 2017 Challenge Setup: Tasks, Datasets and Baseline System",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "85--92",
    abstract = "DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics.",
    keywords = "Sound scene analysis, Acoustic scene classification, Sound event detection, Audio tagging, Rare sound events"
}

@misc{schindler_multi-temporal_2018,
	title = {Multi-{Temporal} {Resolution} {Convolutional} {Neural} {Networks} for {Acoustic} {Scene} {Classification}},
	url = {http://arxiv.org/abs/1811.04419},
	doi = {10.48550/arXiv.1811.04419},
	abstract = {In this paper we present a Deep Neural Network architecture for the task of acoustic scene classiﬁcation which harnesses information from increasing temporal resolutions of Mel-Spectrogram segments. This architecture is composed of separated parallel Convolutional Neural Networks which learn spectral and temporal representations for each input resolution. The resolutions are chosen to cover ﬁne-grained characteristics of a scene’s spectral texture as well as its distribution of acoustic events. The proposed model shows a 3.56\% absolute improvement of the best performing single resolution model and 12.49\% of the DCASE 2017 Acoustic Scenes Classiﬁcation task baseline [1].},
	language = {en},
	urldate = {2025-02-25},
	publisher = {arXiv},
	author = {Schindler, Alexander and Lidy, Thomas and Rauber, Andreas},
	month = nov,
	year = {2018},
	note = {arXiv:1811.04419 [cs]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Multimedia},
	annote = {Comment: In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017), November 2017},
	file = {PDF:/Users/nkdem/Zotero/storage/HTEAW4SF/Schindler et al. - 2018 - Multi-Temporal Resolution Convolutional Neural Networks for Acoustic Scene Classification.pdf:application/pdf},
}


@inproceedings{barker_third_2015,
	title = {The third ‘{CHiME}’ speech separation and recognition challenge: {Dataset}, task and baselines},
	shorttitle = {The third ‘{CHiME}’ speech separation and recognition challenge},
	url = {https://ieeexplore.ieee.org/abstract/document/7404837},
	doi = {10.1109/ASRU.2015.7404837},
	abstract = {The CHiME challenge series aims to advance far field speech recognition technology by promoting research at the interface of signal processing and automatic speech recognition. This paper presents the design and outcomes of the 3rd CHiME Challenge, which targets the performance of automatic speech recognition in a real-world, commercially-motivated scenario: a person talking to a tablet device that has been fitted with a six-channel microphone array. The paper describes the data collection, the task definition and the baseline systems for data simulation, enhancement and recognition. The paper then presents an overview of the 26 systems that were submitted to the challenge focusing on the strategies that proved to be most successful relative to the MVDR array processing and DNN acoustic modeling reference system. Challenge findings related to the role of simulated data in system training and evaluation are discussed.},
	urldate = {2025-03-10},
	booktitle = {2015 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU})},
	author = {Barker, Jon and Marxer, Ricard and Vincent, Emmanuel and Watanabe, Shinji},
	month = dec,
	year = {2015},
	keywords = {'CHiME' challenge, Arrays, microphone array, Microphones, Noise measurement, Noise-robust ASR, Speech, Speech recognition, Training, Training data},
	pages = {504--511},
	file = {Full Text PDF:/Users/nkdem/Zotero/storage/LXQ694EK/Barker et al. - 2015 - The third ‘CHiME’ speech separation and recognition challenge Dataset, task and baselines.pdf:application/pdf},
}


@misc{barker_fifth_2018,
	title = {The fifth '{CHiME}' {Speech} {Separation} and {Recognition} {Challenge}: {Dataset}, task and baselines},
	shorttitle = {The fifth '{CHiME}' {Speech} {Separation} and {Recognition} {Challenge}},
	url = {http://arxiv.org/abs/1803.10609},
	doi = {10.48550/arXiv.1803.10609},
	abstract = {The CHiME challenge series aims to advance robust automatic speech recognition (ASR) technology by promoting research at the interface of speech and language processing, signal processing, and machine learning. This paper introduces the 5th CHiME Challenge, which considers the task of distant multimicrophone conversational ASR in real home environments. Speech material was elicited using a dinner party scenario with efforts taken to capture data that is representative of natural conversational speech and recorded by 6 Kinect microphone arrays and 4 binaural microphone pairs. The challenge features a single-array track and a multiple-array track and, for each track, distinct rankings will be produced for systems focusing on robustness with respect to distant-microphone capture vs. systems attempting to address all aspects of the task including conversational language modeling. We discuss the rationale for the challenge and provide a detailed description of the data collection procedure, the task, and the baseline systems for array synchronization, speech enhancement, and conventional and end-to-end ASR.},
	language = {en},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Barker, Jon and Watanabe, Shinji and Vincent, Emmanuel and Trmal, Jan},
	month = mar,
	year = {2018},
	note = {arXiv:1803.10609 [cs]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Artificial Intelligence},
	file = {PDF:/Users/nkdem/Zotero/storage/SBX83GRF/Barker et al. - 2018 - The fifth 'CHiME' Speech Separation and Recognition Challenge Dataset, task and baselines.pdf:application/pdf},
}
