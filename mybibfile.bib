@misc{WHO2024deafness,
  author = "{WHO}",
  url = "https://www.who.int/news-room/fact-sheets/detail/deafness-and-hearing-loss",
  title = "Deafness and hearing loss",
  year = {2024}
}
@article{Kochkin2010MarkeTrak8,
  author = {Kochkin, Sergei},
  journal = {The Hearing Journal},
  number = {1},
  title = {MarkeTrak {VIII}: Consumer satisfaction with hearing aids is slowly increasing},
  doi = {10.1097/01.HJ.0000366912.40173.76},
  volume = {63},
  year = {2010}
}

@inproceedings{Huwel2020HearDS,
  author = {Hüwel, Andreas and Adiloğlu, Kamil and Bach, Jörg-Hendrik},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  number = {},
  title = {Hearing aid Research Data Set for Acoustic Environment Recognition},
  doi = {10.1109/ICASSP40776.2020.9053611},
  volume = {},
  year = {2020}
}

@article{Korhonen2021WindNoise,
  author = {Korhonen, Petri},
  journal = {Seminars in Hearing},
  number = {3},
  title = {Wind {{Noise Management}} in {{Hearing Aids}}},
  volume = {42},
  year = {2021},
  doi = {10.1055/s-0041-1735133},
}

@article{Stone2002Delays,
  author = {Stone, Michael A. and Moore, Brian C. J.},
  journal = {Ear and Hearing},
  number = {4},
  title = {Tolerable {Hearing Aid Delays}. {II}. Estimation of Limits Imposed During Speech Production},
  volume = {23},
  year = {2002},
  doi = {10.1097/00003446-200208000-00008},
}

@article{Gomez2021MIPS,
  author = {García-Gómez, Joaquín and Gil-Pita, Roberto and Aguilar-Ortega, Miguel and Utrilla-Manso, Manuel and Rosa-Zurera, Manuel and Mohino-Herranz, Inma},
  journal = {Applied Acoustics},
  title = {Linear Detector and Neural Networks in Cascade for Voice Activity Detection in Hearing Aids},
  volume = {175},
  year = {2021},
  doi = {10.1016/j.apacoust.2020.107832}
}
@article{Furness2015HairCell,
  title={Molecular basis of hair cell loss},
  author={Furness, David N.},
  journal={Cell Tissue Research},
  volume={361},
  pages={387--399},
  year={2015},
  publisher={Springer Berlin Heidelberg},
  doi={10.1007/s00441-015-2113-z}
}
@book{
  Wayland2018Phonetics, 
  place={Cambridge}, 
  title={Phonetics: A Practical Introduction}, 
  publisher={Cambridge University Press}, 
  author={Wayland, Ratree}, 
  year={2018},
} 
@article{Nieman2020HearingLoss,
title = {Hearing Loss},
author = {Nieman, Carrie L. and Oh, Esther S.},
journal = {Annals of Internal Medicine},
volume = {173},
number = {11},
pages = {ITC81-ITC96},
year = {2020},
doi = {10.7326/AITC202012010},
url = {https://www.acpjournals.org/doi/abs/10.7326/AITC202012010}
}
@INPROCEEDINGS{Giannoulis2013ASA,
  author={Giannoulis, Dimitrios and Stowell, Dan and Benetos, Emmanouil and Rossignol, Mathias and Lagrange, Mathieu and Plumbley, Mark D.},
  booktitle={21st European Signal Processing Conference (EUSIPCO 2013)}, 
  title={A database and challenge for acoustic scene classification and event detection}, 
  year={2013},
  volume={},
  number={},
  pages={1-5},
}
@INPROCEEDINGS{Pal2013BlindSourceSeparation,
  author={Pal, Madhab and Roy, Rajib and Basu, Joyanta and Bepari, Milton S.},
  booktitle={2013 International Conference Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE)}, 
  title={Blind source separation: A review and analysis}, 
  year={2013},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICSDA.2013.6709849},
  }

@techreport{Hasemann2024PhonakSphere,
  author = {Hasemann Henning, Krylova Alena},
  institution = {Phonak},
  title = {Revolutionary Speech Understanding with Spheric Speech Clarity},
  year = {2024},
  url = {https://www.phonak.com/content/dam/phonak/en/evidence-library/white-paper/technical-paper/PH\_Insight\_SphericSpeechClarity\_210x297\_EN\_028-2684-02\_V1.00.pdf},
}

% https://www.apple.com/uk/newsroom/2025/02/hearing-aid-feature-available-today-with-airpods-pro-2/
@misc{Apple2025Airpods,
  author = {{Apple Inc.}},
  title = {Hearing Aid Feature Available Today with Airpods Pro 2},
  year = {2025},
  url = {https://www.apple.com/uk/newsroom/2025/02/hearing-aid-feature-available-today-with-airpods-pro-2/},
}


@article{levitt_historical_2007,
	title = {A {Historical} {Perspective} on {Digital} {Hearing} {Aids}: {How} {Digital} {Technology} {Has} {Changed} {Modern} {Hearing} {Aids}},
	volume = {11},
	shorttitle = {A {Historical} {Perspective} on {Digital} {Hearing} {Aids}},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC4111501/},
	doi = {10.1177/1084713806298000},
	abstract = {This article provides the author's perspective on the development of digital hearing aids and how digital signal processing approaches have led to changes in hearing aid design. Major landmarks in the evolution of digital technology are identified, ...},
	language = {en},
	number = {1},
	urldate = {2025-03-10},
	journal = {Trends in Amplification},
	author = {Levitt, Harry},
	month = mar,
	year = {2007},
	pmid = {17301334},
	pages = {7},
}


@misc{magicss_hearing_2020,
	title = {Hearing {Aid} {History}: {From} {Ear} {Trumpets} to {Digital} {Technology}},
	shorttitle = {Hearing {Aid} {History}},
	url = {https://www.embs.org/pulse/articles/hearing-aid-history-from-ear-trumpets-to-digital-technology/},
	abstract = {Author(s): Max ValentinuzziIt is said that time marches on, and one thing is certain: Hearing loss marches right along with it. The recorded history of hearing loss goes back hundreds of years, and attempts to correct hearing loss have been in existence since the very first person to cup a hand behind one ear. Continue Reading Hearing Aid History: From Ear Trumpets to Digital Technology},
	language = {en-US},
	urldate = {2025-03-10},
	journal = {IEEE Pulse},
	author = {magicss},
	month = oct,
	year = {2020},
	file = {Snapshot:/Users/nkdem/Zotero/storage/4ZHJRUBB/hearing-aid-history-from-ear-trumpets-to-digital-technology.html:text/html},
}

@inproceedings{DCASE2017challenge,
    Author = "Mesaros, A. and Heittola, T. and Diment, A. and Elizalde, B. and Shah, A. and Vincent, E. and Raj, B. and Virtanen, T.",
    title = "{DCASE} 2017 Challenge Setup: Tasks, Datasets and Baseline System",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "85--92",
    abstract = "DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics.",
    keywords = "Sound scene analysis, Acoustic scene classification, Sound event detection, Audio tagging, Rare sound events"
}

@misc{schindler_multi-temporal_2018,
	title = {Multi-{Temporal} {Resolution} {Convolutional} {Neural} {Networks} for {Acoustic} {Scene} {Classification}},
	url = {http://arxiv.org/abs/1811.04419},
	doi = {10.48550/arXiv.1811.04419},
	abstract = {In this paper we present a Deep Neural Network architecture for the task of acoustic scene classiﬁcation which harnesses information from increasing temporal resolutions of Mel-Spectrogram segments. This architecture is composed of separated parallel Convolutional Neural Networks which learn spectral and temporal representations for each input resolution. The resolutions are chosen to cover ﬁne-grained characteristics of a scene’s spectral texture as well as its distribution of acoustic events. The proposed model shows a 3.56\% absolute improvement of the best performing single resolution model and 12.49\% of the DCASE 2017 Acoustic Scenes Classiﬁcation task baseline [1].},
	language = {en},
	urldate = {2025-02-25},
	publisher = {arXiv},
	author = {Schindler, Alexander and Lidy, Thomas and Rauber, Andreas},
	month = nov,
	year = {2018},
	note = {arXiv:1811.04419 [cs]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Multimedia},
	annote = {Comment: In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017), November 2017},
	file = {PDF:/Users/nkdem/Zotero/storage/HTEAW4SF/Schindler et al. - 2018 - Multi-Temporal Resolution Convolutional Neural Networks for Acoustic Scene Classification.pdf:application/pdf},
}


@inproceedings{barker_third_2015,
	title = {The third ‘{CHiME}’ speech separation and recognition challenge: {Dataset}, task and baselines},
	shorttitle = {The third ‘{CHiME}’ speech separation and recognition challenge},
	url = {https://ieeexplore.ieee.org/abstract/document/7404837},
	doi = {10.1109/ASRU.2015.7404837},
	abstract = {The CHiME challenge series aims to advance far field speech recognition technology by promoting research at the interface of signal processing and automatic speech recognition. This paper presents the design and outcomes of the 3rd CHiME Challenge, which targets the performance of automatic speech recognition in a real-world, commercially-motivated scenario: a person talking to a tablet device that has been fitted with a six-channel microphone array. The paper describes the data collection, the task definition and the baseline systems for data simulation, enhancement and recognition. The paper then presents an overview of the 26 systems that were submitted to the challenge focusing on the strategies that proved to be most successful relative to the MVDR array processing and DNN acoustic modeling reference system. Challenge findings related to the role of simulated data in system training and evaluation are discussed.},
	urldate = {2025-03-10},
	booktitle = {2015 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU})},
	author = {Barker, Jon and Marxer, Ricard and Vincent, Emmanuel and Watanabe, Shinji},
	month = dec,
	year = {2015},
	keywords = {'CHiME' challenge, Arrays, microphone array, Microphones, Noise measurement, Noise-robust ASR, Speech, Speech recognition, Training, Training data},
	pages = {504--511},
	file = {Full Text PDF:/Users/nkdem/Zotero/storage/LXQ694EK/Barker et al. - 2015 - The third ‘CHiME’ speech separation and recognition challenge Dataset, task and baselines.pdf:application/pdf},
}


@article{tzanetakis_musical_2002,
	title = {Musical genre classification of audio signals},
	volume = {10},
	issn = {1558-2353},
	url = {https://ieeexplore.ieee.org/document/1021072/?arnumber=1021072},
	doi = {10.1109/TSA.2002.800560},
	abstract = {Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61\% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.},
	number = {5},
	urldate = {2025-03-11},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Tzanetakis, G. and Cook, P.},
	month = jul,
	year = {2002},
	note = {Conference Name: IEEE Transactions on Speech and Audio Processing},
	keywords = {Computer science, Cultural differences, Feature extraction, Humans, Instruments, Multiple signal classification, Music information retrieval, Pattern recognition, Signal analysis, Wavelet analysis},
	pages = {293--302},
	file = {Full Text PDF:/Users/nkdem/Zotero/storage/53I298XX/Tzanetakis and Cook - 2002 - Musical genre classification of audio signals.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/nkdem/Zotero/storage/K72D7CQR/1021072.html:text/html},
}

@inproceedings{barker18_fifth_2018,
  author={Jon Barker and Shinji Watanabe and Emmanuel Vincent and Jan Trmal},
  title={{The Fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, Task and Baselines}},
  year=2018,
  booktitle={Proc. Interspeech 2018},
  pages={1561--1565},
  doi={10.21437/Interspeech.2018-1768}
}

@inproceedings{mesaros_tut_2016,
	title = {{TUT} database for acoustic scene classification and sound event detection},
	url = {https://ieeexplore.ieee.org/document/7760424/?arnumber=7760424},
	doi = {10.1109/EUSIPCO.2016.7760424},
	abstract = {We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting of binaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.},
	urldate = {2025-03-12},
	booktitle = {2016 24th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
	month = aug,
	year = {2016},
	note = {ISSN: 2076-1465},
	keywords = {Automobiles, Databases, Europe, Event detection, Mel frequency cepstral coefficient, Signal processing},
	pages = {1128--1132},
}

@article{sparkes_study_1997,
	title = {A study of mercuric oxide and zinc-air battery life in hearing aids},
	volume = {111},
	issn = {0022-2151},
	doi = {10.1017/s002221510013871x},
	abstract = {The requirement to phase out mercuric oxide (mercury) batteries on environmental grounds has led to the widespread introduction of zinc-air technology. The possibility arises that high drain hearing aids may not be adequately catered for by zinc-air cells, leading to poor performance. This study investigated the hearing aid user's ability to perceive differences between zinc-air and mercury cells in normal everyday usage. The data was collected for 100 experienced hearing aid users in field trials. Users report 50 per cent greater life for zinc-air cells in high power aids and 28 per cent in low power aids. The average life of the zinc-air cells range from 15 days in high power to 34 days in low power aids. Users are able to perceive a difference in sound quality in favour of zinc-air cells for low and medium power aids. The hearing aid population is not disadvantaged by phasing out mercury cells.},
	language = {eng},
	number = {9},
	journal = {The Journal of Laryngology and Otology},
	author = {Sparkes, C. and Lacey, N. K.},
	month = sep,
	year = {1997},
	pmid = {9373545},
	keywords = {Disposable Equipment, Electric Power Supplies, Equipment Design, Evaluation Studies as Topic, Hearing Aids, Humans, Mercury Compounds, Oxides, Statistics, Nonparametric, Zinc},
	pages = {814--819},
}


@article{mir_evaluation_2023,
	title = {Evaluation of end-of-life zinc-air hearing aid batteries for zinc recovery},
	volume = {198},
	issn = {0892-6875},
	url = {https://www.sciencedirect.com/science/article/pii/S0892687523000961},
	doi = {10.1016/j.mineng.2023.108082},
	abstract = {Primary zinc-air batteries are extensively used in medical, military, and telecommunication applications. The structure of zinc-air batteries consists of a zinc anode (Zn/ZnO), an alkaline electrolyte, and a porous air cathode. The zinc contribution towards the total market value in the zinc-air battery is nearly 60.9\%, higher than alkaline (40.7\%) and zinc-carbon batteries (29.6\%). Therefore, zinc-air batteries are an attractive secondary source for zinc recycling. It is estimated that nearly 8500 tonnes of ore should be mined and processed to meet the annual requirement of zinc-air cells globally. Reissuing new batteries in return for discarded batteries is recommended for streamlining the recycling process. The literature for recycling zinc (Zn/ZnO) from various industrial wastes and primary batteries is systematically reviewed in the context of zinc-air batteries. The zinc-air batteries consist of simpler, easy-to-process zinc phases (Zn/ZnO) without other impurities. A recycling flowsheet is proposed to recycle zinc values from end-of-life zinc-air batteries.},
	urldate = {2025-03-19},
	journal = {Minerals Engineering},
	author = {Mir, Shaila and Vij, Sunali and Dhawan, Nikhil},
	month = jul,
	year = {2023},
	keywords = {Air batteries, Market value, Primary ore, Recycling, Zinc},
	pages = {108082},
	file = {ScienceDirect Snapshot:/Users/nkdem/Zotero/storage/5J5JMY2T/S0892687523000961.html:text/html},
}


@article{thomas_zincair_2024,
	title = {Zinc–{Air} {Hearing} {Aid} {Batteries}: {An} {Analysis} of {Functional} {Performance}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2039-4349},
	shorttitle = {Zinc–{Air} {Hearing} {Aid} {Batteries}},
	url = {https://www.mdpi.com/2039-4349/14/4/56},
	doi = {10.3390/audiolres14040056},
	abstract = {Background: The aim of this study was to evaluate the performance of three disposable hearing aid battery brands available in Wales. Hearing-impaired individuals who utilise hearing aids rely on the functionality of their devices, which is often contingent upon the quality and longevity of disposable batteries. Materials and Methods: A grey literature review foregrounded the battery standards. The “real-life” use of batteries was supplemented through laboratory testing. Parameters relating to performance quality were used to quantify an overall service life of five PR44- and four PR48-size batteries per manufacturer. Results: The literature review signalled a large gap in hearing aid battery consumption research. All battery brands underperformed compared to their specifications but met IEC standards. Conclusions: Revisions to battery consumption test conditions should reflect new technological features and refine expectations of real-life use. It was possible to statistically identify the best performing hearing aid battery brand.},
	language = {en},
	number = {4},
	urldate = {2025-03-19},
	journal = {Audiology Research},
	author = {Thomas, James and Bardsley, Barry and Wild, Jane and Penman, Michael William Owen},
	month = aug,
	year = {2024},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {battery consumption, disposable battery, hearing aid battery, zinc–air battery},
	pages = {659--673},
	file = {Full Text PDF:/Users/nkdem/Zotero/storage/UGFWWRYL/Thomas et al. - 2024 - Zinc–Air Hearing Aid Batteries An Analysis of Functional Performance.pdf:application/pdf},
}

@incollection{schuster-bruce_conventional_2025,
	address = {Treasure Island (FL)},
	title = {Conventional {Hearing} {Aid} {Indications} and {Selection}},
	copyright = {Copyright © 2025, StatPearls Publishing LLC.},
	url = {http://www.ncbi.nlm.nih.gov/books/NBK567712/},
	abstract = {This article will discuss the indications and selection of conventional hearing aids. Conventional hearing aids are non-invasive (not requiring surgery) and are placed behind the pinna, in the canal, or are body-worn. Invasive hearing aids, including bone-anchored hearing aids and cochlear implants, are excluded from coverage in this chapter. Hearing aids, by definition, are sound-amplifying devices that increase the user's ability to detect noise. The components of a non-invasive hearing aid vary widely but broadly consist of a microphone, amplifier, receiver, and battery. The microphone converts external acoustic energy into electrical energy, which is amplified by the amplifier. The receiver detects this and converts it back into acoustic energy, projecting sound into the ear canal. The amplification is driven by the battery, which can be made from zinc-air batteries, mercury, alkaline, or rechargeable batteries. A non-invasive hearing aid aims to increase the sound levels delivered to and hence detected by the hair cells in the cochlea.},
	language = {eng},
	urldate = {2025-03-19},
	booktitle = {{StatPearls}},
	publisher = {StatPearls Publishing},
	author = {Schuster-Bruce, James and Gosnell, Emilee},
	year = {2025},
	pmid = {33620789},
}



@book{loizou_speech_2007,
	address = {Boca Raton},
	title = {Speech {Enhancement}: {Theory} and {Practice}},
	isbn = {978-0-429-13373-2},
	shorttitle = {Speech {Enhancement}},
	abstract = {The first book to provide comprehensive and up-to-date coverage of all major speech enhancement algorithms proposed in the last two decades, Speech Enhancement: Theory and Practice is a valuable resource for experts and newcomers in the field. The book covers traditional speech enhancement algorithms, such as spectral subtraction and Wiener filteri},
	publisher = {CRC Press},
	author = {Loizou, Philipos C.},
	month = jun,
	year = {2007},
	doi = {10.1201/9781420015836},
}


@book{katagiri_handbook_2000,
	address = {USA},
	edition = {1st},
	title = {Handbook of {Neural} {Networks} for {Speech} {Processing}},
	isbn = {978-0-89006-954-7},
	abstract = {From the Publisher:Here are the comprehensive details on cutting edge technologies employing neural networks for speech recognition and speech processing in modern communications. Going far beyond the simple speech recognition technologies on the market today, this new book, written by and for speech and signal processing engineers in industry, R\&amp;D, and academia, takes you to the forefront of the hottest emergent neural net-based speech processing techniques.},
	publisher = {Artech House, Inc.},
	author = {Katagiri, Shigeru},
	month = aug,
	year = {2000},
}

@article{hou_local_2023,
	title = {Local spectral attention for full-band speech enhancement},
	volume = {3},
	issn = {2691-1191},
	url = {https://pubs.aip.org/jel/article/3/11/115201/2919537/Local-spectral-attention-for-full-band-speech},
	doi = {10.1121/10.0022268},
	abstract = {Attention mechanism has been widely used in speech enhancement (SE) because, theoretically, it can effectively model the inherent connection of signal in time domain and spectrum domain. In this Letter, it is found that the attention over the entire frequency range hampers the inference for full-band SE and possibly leads to excessive residual noise and degradation of speech. To alleviate this problem, the local spectral attention is introduced into full-band SE model by limiting the span of attention. The ablation tests on three full-band SE models reveal that the local frequency attention can effectively improve overall performance. CV 2023 Author(s). All article content, except where otherwise noted, is licensed under a Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).},
	language = {en},
	number = {11},
	urldate = {2025-03-16},
	journal = {JASA Express Letters},
	author = {Hou, Zhongshu and Hu, Qinwen and Chen, Kai and Cao, Zhanzhong and Lu, Jing},
	month = nov,
	year = {2023},
	pages = {115201},
	file = {PDF:/Users/nkdem/Zotero/storage/IYQCMN3R/Hou et al. - 2023 - Local spectral attention for full-band speech enhancement.pdf:application/pdf},
}



@article{saleem_time_2024,
	title = {Time domain speech enhancement with {CNN} and time-attention transformer},
	volume = {147},
	issn = {1051-2004},
	url = {https://www.sciencedirect.com/science/article/pii/S1051200424000332},
	doi = {10.1016/j.dsp.2024.104408},
	abstract = {Speech enhancement in the time domain involves improving the quality and intelligibility of noisy speech by processing the waveform directly without the need for explicit feature extraction or domain transformation. Deep learning is a powerful approach for time domain speech enhancement, offering significant improvements over traditional techniques. Formulating a resource-efficient deep neural model in the time domain without ignoring the contextual information and detailed features of input speech is still a vital challenge. To address this challenge, this study proposes a speech enhancement model using 1D-time domain dilated residual blocks in the convolutional encoder-decoder framework. Further, this study integrates a time-attention transformer (TAT) bottleneck between the encoder-decoder. The TAT model extends the transformer architecture by incorporating a time-attention mechanism, which enables the model to selectively attend to different segments of the speech signal over time. This allows the model to effectively capture long-term dependencies in the speech and learn to recognize important features. The experimental results indicate that the proposed speech enhancement outperforms the recent deep neural networks (DNNs) and substantially improves the intelligibility and quality of noisy speech. With the WSJ0 SI-84 database, the proposed SE improves the STOI and PESQ by 21.51\% and 1.14 over noisy speech.},
	urldate = {2025-03-16},
	journal = {Digital Signal Processing},
	author = {Saleem, Nasir and Gunawan, Teddy Surya and Dhahbi, Sami and Bourouis, Sami},
	month = apr,
	year = {2024},
	keywords = {Transformer, Convolutional encoder-decoder, Time attention, Time-domain speech enhancement},
	pages = {104408},
	file = {ScienceDirect Full Text PDF:/Users/nkdem/Zotero/storage/ALQFRDBB/Saleem et al. - 2024 - Time domain speech enhancement with CNN and time-attention transformer.pdf:application/pdf;ScienceDirect Snapshot:/Users/nkdem/Zotero/storage/SX6P9BWM/S1051200424000332.html:text/html},
}


@misc{dai_very_2016,
	title = {Very {Deep} {Convolutional} {Neural} {Networks} for {Raw} {Waveforms}},
	url = {http://arxiv.org/abs/1610.00087},
	doi = {10.48550/arXiv.1610.00087},
	abstract = {Learning acoustic models directly from the raw waveform data with minimal processing is challenging. Current waveform-based models have generally used very few (∼2) convolutional layers, which might be insufﬁcient for building high-level discriminative features. In this work, we propose very deep convolutional neural networks (CNNs) that directly use time-domain waveforms as inputs. Our CNNs, with up to 34 weight layers, are efﬁcient to optimize over very long sequences (e.g., vector of size 32000), necessary for processing acoustic waveforms. This is achieved through batch normalization, residual learning, and a careful design of down-sampling in the initial layers. Our networks are fully convolutional, without the use of fully connected layers and dropout, to maximize representation learning. We use a large receptive ﬁeld in the ﬁrst convolutional layer to mimic bandpass ﬁlters, but very small receptive ﬁelds subsequently to control the model capacity. We demonstrate the performance gains with the deeper models. Our evaluation shows that the CNN with 18 weight layers outperform the CNN with 3 weight layers by over 15\% in absolute accuracy for an environmental sound recognition task and matches the performance of models using log-mel features.},
	language = {en},
	urldate = {2024-12-29},
	publisher = {arXiv},
	author = {Dai, Wei and Dai, Chia and Qu, Shuhui and Li, Juncheng and Das, Samarjit},
	month = oct,
	year = {2016},
	note = {arXiv:1610.00087 [cs]},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 5 pages, 2 figures, under submission to International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2017},
	file = {PDF:/Users/nkdem/Zotero/storage/7M6BXF2I/Dai et al. - 2016 - Very Deep Convolutional Neural Networks for Raw Waveforms.pdf:application/pdf},
}

@inproceedings{kumar_end_2020,
	address = {Bangalore, India},
	title = {End-to-end audio-scene classification from raw audio: {Multi} time-frequency resolution {CNN} architecture for efficient representation learning},
	shorttitle = {End-to-end audio-scene classification from raw audio},
	url = {https://ieeexplore.ieee.org/abstract/document/9179600},
	doi = {10.1109/SPCOM50965.2020.9179600},
	abstract = {We propose and study a novel multi-temporal CNN architecture for end-to-end audio-scene classification (ASC) from raw audio signal. Conventional CNNs use a fixed size kernel (whether for image or 1-d signal classification) which corresponds to applying a filter bank, where each filter has a fixed time-frequency resolution (i.e., fixed duration impulse response and a fixed band-width frequency response), importantly with a specific time-frequency trade-off. In contrast, in a way to allow for multiple time-frequency resolutions, we use a multi-temporal CNN architecture having multiple kernel branches (up to 12 branches) each of different lengths, thereby allowing for multiple filter banks with different time-frequency resolution to process the input raw audio signal and create feature-maps (e.g. ranging from very narrow-band to very wide-band spectrographic maps in steps of fine time-frequency resolutions) corresponding to different time-frequency trade-offs. Applying this architecture to end-to-end audio-scene classification is shown to offer consistent and significant performance enhancements (e.g. 11-15 percent absolute in accuracy for the multi-temporal case of 12 branches) over the conventional single-temporal CNN and also outperform state-of the-art results for this task.},
	booktitle = {2020 International Conference on Signal Processing and Communications (SPCOM)},
	publisher = {IEEE},
	author = {Kumar, T. Vijaya and Sundar, R. Shunmuga and Purohit, Tilak and Ramasubramanian, V.},
	month = jul,
	year = {2020},
	pages = {1--5}
}
@book{prince2023understanding,
        author = "Simon J.D. Prince",
        title = "Understanding Deep Learning",
        publisher = "The MIT Press",
        year = 2023,
        url = "http://udlbook.com"
}

@misc{an_ensemble_2020,
	title = {An {Ensemble} of {Simple} {Convolutional} {Neural} {Network} {Models} for {MNIST} {Digit} {Recognition}},
	url = {http://arxiv.org/abs/2008.10400},
	doi = {10.48550/arXiv.2008.10400},
	abstract = {We report that a very high accuracy on the MNIST test set can be achieved by using simple convolutional neural network (CNN) models. We use three different models with 3×3, 5×5, and 7×7 kernel size in the convolution layers. Each model consists of a set of convolution layers followed by a single fully connected layer. Every convolution layer uses batch normalization and ReLU activation, while pooling is not used. Rotation and translation is used to augment training data, which is a technique frequently used in most image classiﬁcation tasks. A majority voting using the three models independently trained on the training set can achieve up to 99.87\% accuracy on the test set, which is one of the state-of-the-art results. A two-layer ensemble, a heterogeneous ensemble of three homogeneous ensemble networks, can achieve up to 99.91\% test accuracy. The results can be reproduced by using the code at https://github.com/ansh941/MnistSimpleCNN.},
	language = {en},
	urldate = {2025-03-21},
	publisher = {arXiv},
	author = {An, Sanghyeon and Lee, Minjun and Park, Sanglee and Yang, Heerin and So, Jungmin},
	month = oct,
	year = {2020},
	note = {arXiv:2008.10400 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 10 pages, 12 figures, 7 tables},
	file = {PDF:/Users/nkdem/Zotero/storage/MV7YUGGI/An et al. - 2020 - An Ensemble of Simple Convolutional Neural Network Models for MNIST Digit Recognition.pdf:application/pdf},
}


@inproceedings{liu_simple_2023,
	title = {Simple {Pooling} {Front}-{Ends} for {Efficient} {Audio} {Classification}},
	url = {https://ieeexplore.ieee.org/document/10096211},
	doi = {10.1109/ICASSP49357.2023.10096211},
	abstract = {Recently, there has been increasing interest in building efficient audio neural networks for on-device scenarios. Most existing approaches are designed to reduce the size of audio neural networks using methods such as model pruning. In this work, we show that instead of reducing model size using complex methods, eliminating the temporal redundancy in the input audio features (e.g., mel-spectrogram) could be an effective approach for efficient audio classification. To do so, we proposed a family of simple pooling front-ends (SimPFs) which use simple non-parametric pooling operations to reduce the redundant information within the mel-spectrogram. We perform extensive experiments on four audio classification tasks to evaluate the performance of SimPFs. Experimental results show that SimPFs can achieve a reduction in more than half of the number of floating point operations (FLOPs) for off-the-shelf audio neural networks, with negligible degradation or even some improvements in audio classification performance.},
	urldate = {2025-03-21},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Liu, Xubo and Liu, Haohe and Kong, Qiuqiang and Mei, Xinhao and Plumbley, Mark D. and Wang, Wenwu},
	month = jun,
	year = {2023},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Audio classification, audio front-ends, Buildings, Computational efficiency, convolutional neural networks, deep learning, Degradation, Neural networks, on-device, Redundancy, Task analysis},
	pages = {1--5},
	file = {Full Text PDF:/Users/nkdem/Zotero/storage/MK5XM9D5/Liu et al. - 2023 - Simple Pooling Front-Ends for Efficient Audio Classification.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/nkdem/Zotero/storage/9LYR4N2W/10096211.html:text/html},
}


@inproceedings{valentini-botinhao_speech_2016,
	title = {Speech {Enhancement} for a {Noise}-{Robust} {Text}-to-{Speech} {Synthesis} {System} {Using} {Deep} {Recurrent} {Neural} {Networks}},
	url = {https://www.isca-archive.org/interspeech\_2016/valentinibotinhao16\_interspeech.html},
	doi = {10.21437/Interspeech.2016-159},
	abstract = {Quality of text-to-speech voices built from noisy recordings is diminished. In order to improve it we propose the use of a recurrent neural network to enhance acoustic parameters prior to training. We trained a deep recurrent neural network using a parallel database of noisy and clean acoustics parameters as input and output of the network. The database consisted of multiple speakers and diverse noise conditions. We investigated using text-derived features as an additional input of the network. We processed a noisy database of two other speakers using this network and used its output to train an HMM acoustic text-to-synthesis model for each voice. Listening experiment results showed that the voice built with enhanced parameters was ranked signiﬁcantly higher than the ones trained with noisy speech and speech that has been enhanced using a conventional enhancement system. The text-derived features improved results only for the female voice, where it was ranked as highly as a voice trained with clean speech.},
	language = {en},
	urldate = {2025-03-23},
	booktitle = {Interspeech 2016},
	publisher = {ISCA},
	author = {Valentini-Botinhao, Cassia and Wang, Xin and Takaki, Shinji and Yamagishi, Junichi},
	month = sep,
	year = {2016},
	pages = {352--356},
}


@misc{kim_specmix_2021,
	title = {{SpecMix} : {A} {Mixed} {Sample} {Data} {Augmentation} method for {Training} {withTime}-{Frequency} {Domain} {Features}},
	shorttitle = {{SpecMix}},
	url = {http://arxiv.org/abs/2108.03020},
	doi = {10.48550/arXiv.2108.03020},
	abstract = {A mixed sample data augmentation strategy is proposed to enhance the performance of models on audio scene classiﬁcation, sound event classiﬁcation, and speech enhancement tasks. While there have been several augmentation methods shown to be effective in improving image classiﬁcation performance, their efﬁcacy toward time-frequency domain features of audio is not assured. We propose a novel audio data augmentation approach named ”Specmix” speciﬁcally designed for dealing with time-frequency domain features. The augmentation method consists of mixing two different data samples by applying time-frequency masks effective in preserving the spectral correlation of each audio sample. Our experiments on acoustic scene classiﬁcation, sound event classiﬁcation, and speech enhancement tasks show that the proposed Specmix improves the performance of various neural network architectures by a maximum of 2.7\%.},
	language = {en},
	urldate = {2025-03-17},
	publisher = {arXiv},
	author = {Kim, Gwantae and Han, David K. and Ko, Hanseok},
	month = aug,
	year = {2021},
	note = {arXiv:2108.03020 [cs]},
	keywords = {Computer Science - Sound},
	annote = {Comment: Accepted to interspeech 2021},
	file = {PDF:/Users/nkdem/Zotero/storage/XTX6WHKN/Kim et al. - 2021 - SpecMix  A Mixed Sample Data Augmentation method for Training withTime-Frequency Domain Features.pdf:application/pdf},
}
