% UG project example file, February 2024
%
%   Added the "online" option for equal margins, February 2024 [Hiroshi Shimodaira, Iain Murray]
%   A minor change in citation, September 2023 [Hiroshi Shimodaira]
%
% Do not change the first two lines of code, except you may delete "logo," if causing problems.
% Understand any problems and seek approval before assuming it's ok to remove ugcheck.
\documentclass[logo,bsc,singlespacing,parskip,online]{infthesis}
\usepackage{ugcheck}


% Include any packages you need below, but don't include any that change the page
% layout or style of the dissertation. By including the ugcheck package above,
% you should catch most accidental changes of page layout though.

\usepackage{microtype} % recommended, but you can remove if it causes problems
\usepackage[square,numbers]{natbib} % recommended for citations
\usepackage{graphicx}
\graphicspath{ {./images/}}
\bibliographystyle{unsrtnat}

\begin{document}
\begin{preliminary}

\title{Investigating Machine Learning Techniques for Hearing Aids}

\author{Nikodem Bieniek}

%\course{Artificial Intelligence and Computer Science}
\course{Master of Informatics} % MInf students

\project{MInf Project (Part 1) Report}  % 4th year MInf students
%\project{MInf Project (Part 2) Report}  % 5th year MInf students


\date{\today}

% TOOD: Need to tidy up and expand the abstract
\abstract{
   This dissertation takes the intersection of Machine Learning, Automatic Speech Recognition (ASR) and Hearing Aids (HA) 
   to evaluate the effectiveness of machine learning algorithms in prioritising speech in noisy environments. 
   The aim is to classify the environment based on the audio signal, and then apply heuristic speech enhancement techniques to 
   prioritise speech in particular environments. The experiments are conducted on a novel dataset called HEAR-DS where 
   we evaluate the baseline Convolutional Neural Network (CNN) classifier presented by the authors of the dataset. 
   We then demonstrate techniques that speed up the training process of the CNN model, by using the Adam optimiser. 
   It is also shown that the model can generalise better by applying data augmentation techniques.
   We then compare the performance of the CNN model to a Recurrent Neural Network (RNN) model, specifically 
   the Long Short-Term Memory (LSTM). The results show that the LSTM model outperforms the CNN model. 


   Moreover, we develop a baseline speech enhancement model using a basic CNN model. We then take inspiration 
   from the literatue and apply more layers to the model to improve the performance. We then compare the performance 
   by comparing the STOI and PESQ scores of the models. The results show that the more complex model outperforms the
   baseline model. 
}

\maketitle

\newenvironment{ethics}
   {\begin{frontenv}{Research Ethics Approval}{\LARGE}}
   {\end{frontenv}\newpage}

\begin{ethics}
% \textbf{Instructions:} \emph{Agree with your supervisor which
% statement you need to include. Then delete the statement that you are not using,
% and the instructions in italics.\\
% \textbf{Either complete and include this statement:}}\\ % DELETE THESE INSTRUCTIONS
% %gg
% % IF ETHICS APPROVAL WAS REQUIRED:
% This project obtained approval from the Informatics Research Ethics committee.\\
% Ethics application number: ???\\
% Date when approval was obtained: YYYY-MM-DD\\
% %
% \emph{[If the project required human participants, edit as appropriate, otherwise delete:]}\\ % DELETE THIS LINE
% The participants' information sheet and a consent form are included in the appendix.\\
% %
% % IF ETHICS APPROVAL WAS NOT REQUIRED:
% \textbf{\emph{Or include this statement:}}\\ % DELETE THIS LINE
This project was planned in accordance with the Informatics Research
Ethics policy. It did not involve any aspects that required approval
from the Informatics Research Ethics committee.

\standarddeclaration
\end{ethics}


\begin{acknowledgements}
First and foremost, I would like to solemly thank my supervisor, Hao Tang for 
his enthusiasm in guiding me through this self-proposed dissertation. I am eternally grateful for his wisdom, guidance, and patience throughout the project.

And to my parents, without whose sacrifices I would not have been able to pursue this degree, let alone this project.
\end{acknowledgements}


\tableofcontents
\end{preliminary}


\chapter{Introduction}
\section{Motivations}
Hearing loss is a prevalent condition affecting as much as 
430 million people - or 1 in 18 people. This is expected 
to rise to 1 in 10 by 2050 \cite{WHO2024deafness}.
The most common treatment for hearing loss is the 
provision of hearing technology - such as Hearing Aid (HA) or cochlear implants.
There are many types of HAs, but the most common
type is the behind-the-ear (BTE) hearing aid \cite{Kochkin2010MarkeTrak8}.
However, HA users often report that they struggle to hear speech
in noisy environments. For example, in a study by Kochkin \cite{Kochkin2010MarkeTrak8},
42\% of HA users reported that wind noise was a significant issue for them.
This project aims to evaluate the effectiveness of machine learning algorithms
in prioritising the speech in various environments (such as windy environments).

Modern hearing aids now apply a wide range of techniques to achieve 
better speech prioritisation. For wind noise reduction, this can be achieved 
from mechanical solutions - product design to covers that reduce wind noise - to 
signal processing techniques to compensate for mechanical limitations.
However, current techniques are still not perfect as shown by the study from Kochkin.

Wind noise reduction and indeed, noise reduction in general, is a challenging problem
when paired with speech. This is because you have 
to strike a balance between reducing background noise and
preserving speech. 
Korhen's paper \cite{Korhonen2021WindNoise} outlines 
various techniques that could be used to reduce the wind noise in hearing aids -
from modulation-based noise reduction algorithms (Wiener filtering),
adaptive filtering algorithms, to machine learning techniques.
The paper mentions that the the proposed ML technique:
Long Short-Term Memory (LSTM) neural networks provided
modest improvements in wind noise reduction, however, it did highlight
that ML techniques may still have utility through further research 
and careful algorithmic choices. 

This project aims to pair the proposed machine learning techniques with
signal processing techniques to evaluate the effectiveness of 
speech prioritisation in noisy environments. The idea is to first 
perform acoustic scene analysis (ASA) to classify the environment
based on the audio signal. Afterwards, speech enhancement 
techniques will be applied to the signal to prioritise speech.
ASA can be done using a dataset of varying environments, and 
using machine learning techniques to classify the environment.

In this project, we will be using a novel dataset proposed by \citet{Huwel2020HearDS}
This dataset (called HEAR-DS) is unique because it is specially tailored for HA signal processing and contains
various environments. Normally, voice activity detection (VAD) is used to
detect speech, however, the dataset helpfully contains 
labels which samples contain speech. This can be used 
to implicitly train the machine learning model to classify the environment 
and whether speech is present. Additionally, the paper presents an 
elementary example of how the dataset can be used: to classify the environment -
it showcases the use of a
convolutional neural network (CNN) to classify the environment.
This project will be extending the paper by actually using the dataset 
and comparing various machine learning techniques to evaluate the
effectiveness of the proposed machine learning techniques. 

This project will investigate how recurrent neural networks (RNN) and 
their variants, such as LSTMs, can be used to classify the environment.
 Based on the environment classified, the system will then apply
signal processing techniques to enhance the speech. 
The project 
however has to be mindful in its algorithmic choices - as the computational
power required in HA is limited. It is difficult to pinpoint the exact computational power of a HA
due to the proprietary nature of the devices. In August 2024,
Phonak (Sonova Holding AG) released a new HA which is their first AI 
equipped HA. The device is said to be capable of handling 7,700 Million 
Operations Per Second to accommmodate its neural network with 4.5 million parameters \cite{Hasemann2024PhonakSphere}.  Contrast this with a paper 
from 2021 investigating techniques in VAD for hearing aids 
quotes that it `rarely exceeds 5 million instructions per second (MIPS)` \cite{Gomez2021MIPS}
Moreover, Apple's release of a FDA approved hearing aid in which was previously a mainstream earphone wearable,
Apple Airpods, also shows that there's more interest in this area.
So suffice to say, the computational power of hearing aids is accelerating 
and is most likely going to continue to grow. 

Delay constraints are also
important in HA, as the user needs to hear the speech in real-time.
For example, so long as the speech production is no higher 
than 30 milliseconds (ms), and ideally less than 20ms, the user
is unaffected by the delay \cite{Stone2002Delays}.


% Thus, the evaluation plays a crucial role in this project. From
% objective measures such as the short-time objective intelligibility (STOI)
% and perceptual evaluation of speech quality (PESQ), to quantitative
% measures such as multiply-accumulate operations (MACs) and delay requirements or
% real time factor (RTF) i.e. how fast a system can compute a speech result. 
% The combination of these measures will provide a comprehensive evaluation
% of the proposed machine learning techniques.
%

The project is predominantly aimed at the hearing aid industry. If successful, the project
could advance the state-of-the-art techniques in hearing aids.
Which in turn could improve the quality of life for hearing aid users.
A secondary goal is to make hearing aid research more accessible to the 
computer science community.
The project also hopes to benefit other fields that deal with audio signal processing,
such as mainstream wearables like headphones or microphones.

% \section{Contributions}


\section{Structure}
% The paper is structured as follows:
% \begin{itemize}  
%    \item \textbf{Background} - This chapter will give a high level overview of hearing loss, the auditory system, and the speech processing concepts needed to understand the project.
%    We briefly also cover the two techniques that will be used in this project: Acoustic Scene Analysis (ASA) and Speech Enhancement
%    and some of the related work done in the field. Lastly, I mention the criticism of the paper by \citet{Huwel2020HearDS} 
%    that I will be exploring in this project.
%    \item \textbf{Experimental Setup} - This chapter will cover the datasets that will be used in this project, the baseline models that will be used, and the hypotheses that will be tested.
%    \item We then move onto the results of the experiments.
%    \begin{itemize}
%       \item \textbf{Acoustic Scene Classification} - This section will cover the results of the experiments pertaining to the ASA task.
%       \item \textbf{Speech Enhancement} - This section will cover the results of the experiments pertaining to the speech enhancement task.
%    \end{itemize}
%    \item \textbf{Discussion/Conclusion} - This chapter will discuss the results of the experiments and the implications of the results.
% \end{itemize}
 

\chapter{Background}
% We will begin by briefly giving a high level overview of hearing loss, before discussing 
% what the current treatment and future of hearing loss treatment looks like. We will 
% end on the techniques used to enhance speechV

\section{Hearing Loss}
\subsection{Auditory System}
For sound to be registered by humans, it has to travel through the ear to transform them into
what is known as a neural impulse which is then trasmitted to the brain. We will 
give a high level overview of the process which was collated by the phonetics textbook from \citet{Wayland2018Phonetics},
 but for further reading, refer to the textbook. 
 This process of sound to be registered by humans, is all done in what's known as the Auditory System.
 Figure \ref{fig:ear} shows a birds eye view of the auditory system, and it can be split into three segments
 that we will explore below.

\begin{figure}[h]
   \centering
   \includegraphics[width=0.5\textwidth]{wayland-ear}
   \caption{The external, middle and the inner ear from \citet{Wayland2018Phonetics}}
   \label{fig:ear}
\end{figure}

\subsubsection{The Outer Ear}
The outer ear (sometimes referred to as the External Ear) is responsible for channeling sound waves to the tympanic membrane (ear drum). Firstly,
sound waves reach the Pinna which funnels it through the auditory canal, and at the end of the canal 
is the ear drum, at which point the sound waves collide with the ear drum which causes the ear drum to vibrate. 
The vibrations are passed along to the middle ear.

\subsubsection{The Middle Ear}
Zooming into the middle ear (Figure \ref{fig:middle-ear}), there are three bony structures: the Malleus, Incus, and Stapes,
and together they make up a lever system. 
It is worth pointing out that when transmitting the energy from the ear drum to the middle ear, there is bound to be some energy 
loss \footnote{Depending on the frequency, this can be as high as 40\%}. The lever system's mechanical advantage 
allows for the sound energy to be amplified so the loss is compensated. 
The Stapes also has an additional function besides passing over the sound vibrations to the inner ear. The Stapes 
is connected by a muscle, the Stapedius muscle, and in response to loud noises, it temporarily increases the stifness
of the bones in the middle ear, which temporarily prevents the acoustic energy to be amplified. This 
protects the inner ear from loud noises.

\begin{figure}[h]
   \centering
   \includegraphics[width=0.5\textwidth]{wayland-middle-ear.png}
   \caption{The components of the middle ear from \citet{Wayland2018Phonetics}}
   \label{fig:middle-ear}
\end{figure}

\subsubsection{The Inner Ear}
One of the parts of the inner ear, is the Cohlea, and it is what gives humans the ability to hear sounds.
The cohlea is a bony structure that resemblers that of a snail shell \footnote{'Cohlea' in Latin translates to snail shell!}.
If the cohlea was unrolled its length would be about 3.5cm, and it is subdivided into various parts (see Figure \ref{fig:cohlear}).
For our purposes, it suffices to know that in the Basilar membrane are a collection of cells, called 
the Organ of Corti, which contain hair cells, and that different hair cells register different frequencies.
Figure \ref{fig:corti} shows the frequency responses shows the various areas of the corti 
and its response to different frequencies. The Organ of Corti is linked to the audtiory nerves and 
so the oscillations that pass through it get coverted to neural impulses and transmitted to the brain.


\begin{figure}[h]
   \centering
   \includegraphics[width=0.5\textwidth]{wayland-cohlear.png}
   \caption{'Unrolled' cohlear from \citet{Wayland2018Phonetics}}
   \label{fig:cohlear}
\end{figure}

\begin{figure}[h]
   \centering
   \includegraphics[width=0.5\textwidth]{wayland-corti.png}
   \caption{The Organ of Corti and the frequency responses. From \citet{Wayland2018Phonetics}}
   \label{fig:corti}
\end{figure}

\newpage

% \subsection{Congenital Hearing Loss}
% TODO
% \subsection{Exposure to Loud Noises}
% TODO
% \subsection{Ageing}
% TODO
\subsection{Hair Cell Loss and Hearing Loss}
As we have seen, the hair cells play a major role in allowing humans to register sound. Unfortunately,
it is the major cause of hearing loss since those hair cells are suspectible to damage.
The cause of hair cell loss is complex, since it could be due to many factors such as 
genetic abnormalities (congenital hearing loss), infection, diseases or extrinistic factors such as exposure to loud noises.
It can also occur due to aging, which could be due to the reasons mentioned above, but also 
could be triggered due to age related hearing loss genes. To make matters worse, hair cell loss is unrecoverable in humans \cite{Furness2015HairCell}.
Hearing loss is a spectrum, and varies from mild (26-40dB loss), moderate (41-60dB loss), severe (61-80dB loss) to profound($>80$dB loss) \cite{Nieman2020HearingLoss}.
Moreover, hearing loss may occur in one ear (unilateral) or in both ears (binaural). 

\section{Hearing Loss Treatment}

\subsection{History of Hearing Technology}
Users were fitted with what would have been considered a HA in this day and age as early as the 18th century.






\subsection{Hearing Technology Now}

Hearing aids are increasingly becoming more sophisticated and are starting to incorporate more powerful 
algorithms to improve the quality of life for hearing aid users. From Apple's FDA approved hearing aid in 
their Airpods, to big traditional players like Phonak, Oticon who are starting to incorporate 
DNN and AI into their devices. There is also a trend of rechargable hearing aids, and while 
it has a practical aspect i.e. the user does not need to change the batteries, it's also 
specifically used because of the ability to handle higher peak power consumption, whereas 
non-rechargable hearing aids are restricted to what's allowed by the battery. The problem

As mentioned in the Introduction, the most common treatment for hearing loss 
is the provision of hearing technology. There are two common approaches 
to fitting of hearing technology: the equipment of a Hearing Aid (HA) 
or the fitting of a Cohlear Implant. There are similarities in the 
two, and both can be used to fit users that are experiencing profound hearing loss, 
though typically cohlear implants are only considered if the user is experiencing 
a severe hearing loss or further. Another consideration is cost,
the fitting of a hearing aid is a lower barrier of entry since there 
does not need to be any surgical operations done. 

Due to the proprietary nature of the devices, it is unclear what sort of 
algorithms are used in the devices. Though, from Phonak's white paper on Spheric Speech Clarity \cite{Hasemann2024PhonakSphere}
we can deduce that the devices use a combination of ASA and speech enhancement techniques.
From the paper, before signal processing begins, the sound is fed into Autosense OS which 
performs 'scene classification' so in the context of this project, this can be 
considered as an acoustic scene analysis (ASA) algorithm. More specifically, if 
the signal is classified as 'Speech in Loud Noise', then the signal is fed into a 
deep neural network which outputs a mask that seperates the speech signal from the background noise 
which is then applied to the signal. In the context of this project, we will aim to do something similar 
when we train a speech enhancement model on HEAR-DS. 



% It is enough 
% to know for now that the devices will contain multiple microphones, and 
% a dedicated chip which does further processing on the sound depending 
% on the environment the user is faced with.
% It should be pointed out that there is an active area 
% of research into curing congenitial hearing loss - individuals who have been diagnosed with 
% a hearing loss since birth.
% \subsubsection{Hearing Aids}
% Figure ... shows a typical hearing aid, and Figure ... shows the hearing aid labelled.
% TODO
% Microphone info, the reason for multiple mics etc
% There is a surge into rechargable hearing aids.
% Smaller model = higher battery life 

% \subsection{New Approaches to Treatment}
% \subsubsection{Gene-Based Therapy}
% Genitic mutations account for 70-75\% of congenital hearing loss, and with the advent 
% of gene-based therapy, it is not surprising to see that this avenue is explored with hearing loss. 
% However, besides it being an active area of research, it will not be applicable 
% to individuals that have 

\section{Speech Processing Techniques}
\subsection{Digitisation}
To be able to perform speech processing on a device containing a chip/processor (such as a HA), requires us 
to have some sort of digital representation of the sound wave. A waveform is just that, a digital representation 
of the sound we hear. Digital devices can't have infinite precision, so the signal produced by the sound will 
be divided into discrete samples, and the rate at which this occurs is known as the sampling rate. Additionally,
the precision of the amplitude of the signal will be quantized into discrete numbers, and the precision is 
dictated by the bit depth (or the quantization rate).

How do we choose these two values, you may ask? The sampling 
rate will vary between different speech tasks, and part of the decision 
is dictated by a theorem known as the Nyquist-Shannon Sampling Theorem, 
which states that the sampling rate should be at least twice the 
highest frequency range present in the signal. In other words, 
if your speech processing task involves working with frequencies 
of 8KHz, then the sampling rate should be at least 16KHz.
As for the selection of an appropriate bit depth value, 
this depends on the need of representing a wide dynamic range 
of air pressure in a speech signal. Typically, it is 
common to use 16 bits as it can represent an amplitude range of 96dB.

\subsection{Engineered Acoustic Features}
The prevalance of engineering acoustic features in previous work relating to my project and speech processing in general,
warrants a high level overview of this concept to better understand its usage. 
Given some signal $x(t)$, an engineered feature $f(x(t))$ is a function that transforms the signal in some way.
In this project we will be feeding the models the log mel spectrogram of the signal. To obtain the log mel spectrogram, 
we first need to convert the signal (waveform) into the frequency domain (spectrogram).
The process of converting the signal into the frequency domain is done by utilising 
the Fourier Transform. We will technically be using the Fast Fourier Transform (FFT) 
due to efficiency, and the main hyperparameters for the FFT are the window length $w$ 
and the hop length $h$. The window length is the length of the window that is used to 
split the signal into frames, and the hop length is the number of samples between the 
start of each frame. Additionally, there is a choice of a window function and we will 
be using the most common one, the Hann window. Once we apply the FFT, we will be 
left with a complex valued vector of length $w$ and the magnitude of this vector 
gives us the amplitude of the signal at different frequencies. 
We can then apply a Mel Filterbank - a filterbank that is used to mimic the human auditory system. 
Afterwards, we take the logarithm of the filterbank outputs and we are left with the log mel spectrogram 
which mimics the human perception of loudness. Diagram \ref{fig:log-mel-spectrogram} shows the process to make it more clear.
The reason it is worthwhile to use an engineered feature is for many reasons, but one of the main ones is that 
it is a compact representation of the signal which reduces the amount of memory and computational resources required 
to process the signal which is especially important for embedded devices. The other reason is that it is much 
more clearer to distinguish between speech and background noise from this representation than from the waveform, so 
it is easier for the model to learn.

% In this project we wil be employing the Mel-Frequency Cepstral Coefficients (MFCCs) as our engineered feature.
% The process of extracting MFCSS from waveforms is known as Cepstral Analysis and below are the steps:
% \begin{enumerate}
%    \item Pre-emphasis: Enhances high-frequency components by boosting the signal.
%    \item Windowing: Segments the signal into frames to capture the time-varying nature of speech.
%    \item Fast Fourier Transform (FFT): Converts the signal from the time domain to the frequency domain.
%    \item Mel Filterbank: Applies a filterbank to the FFT output to mimic the human auditory system.
%    \item Logarithm: Converts the filterbank outputs to a logarithmic scale to mimic human perception of hearing.
%    \item Discrete Cosine Transform (DCT): Extracts the cepstral coefficients from the filterbank outputs.
% \end{enumerate}

% \subsection{Acoustic Scene Analysis (ASA)}
% The first step will be 

% \subsection{Blind Source Seperation}

% Blind source seperation 


\section{Related Work}
As this project will chain two algorithms—Acoustic Speech Analysis (ASA) and Speech Prioritization, which is also referred to as Blind Source Separation in some literature—we will cover each in separate sections.
We will discuss the work that has been done and my plans for further development.

\subsection{Acoustic Scene Analysis (ASA)}
Acoustic Scene Analysis is the process of classifying the scene (environment) from an audio stream. For this project 
we will be using a supervised machine learning model for ASA. As such, we need
a sufficiently large dataset that contains useful metadata on the recordings such as the environment it is in.
This project aims to use the HEAR-DS dataset presented by \citet{Huwel2020HearDS}. According to the researchers, 
the dataset came to fruition because existing acoustic scene classification databases are inappropriate for 
HA processing. We will go into more detail of the dataset in Chapter \ref{sec:datasets}.
The paper where the dataset was first introduced, presents the applicability 
of the dataset by showcasing the use of a Convolutional Neural Network (CNN) to classify the environment.
In Chapter \ref{chap:acoustic-scene-classification}, we will be attempting to reproduce the results of the paper. 
As you will see, I struggled to reproduce the results of the paper and I discuss my findings in the chapter too.
So as a validation of the model implementation, I decided to see if the same model parameters 
can be used to classify the DCASE 2017 Acoustic Scenes Challenge dataset \cite{DCASE2017challenge}.
More specifically, I compare \citet{schindler_multi-temporal_2018} model to the CNN model used in the paper.
I discuss my findings in \ref{sec:DCASE-results}.


% But it is enough to know that the dataset is classified in an environment of which it is subcategorised as 
% either containing speech only, background only or speech \& background (both). To prevent machine learning 
% models from overfitting and to increase the diversity of situations, each environment enforces a minimum of three recording situations (REC-SITs) i.e.
% there must be at least three different locations for the recordings. The researchers presented the applicability 
% of this dataset by performing a series of classification experiments using various Convolutional Neural Networks (CNN) of differing sizes.
% This is where the paper could be more clear on the design decisions of the CNN, esepcially because the accuracy of the classification of the models 
% can vary as high as 20\%. This project hopes to reproduce the results, and explain the high variability in the accuracy and how those can be circumvented,
% because the high variability suggests that the random initialisation of weights during training is the primary factor influencing the model performance.
% [To Hao: This is what I understood from your interpretation in the discussion of this paper last Friday, did I understand you correctly?].
% For the recordings labeled as being in environments containing both speech and background noise, the researchers decided against recording the samples in situ.
% Instead they took the speech data from CHiME 
\subsection{Speech Enhancement}

\section{Criticism of Previous Work}
During the course of this project, I found out that the paper by \citet{Huwel2020HearDS} had some issues.
Granted, the paper was more of an exploration of the applicability of the dataset rather than a thorough analysis of the 
performance of the model, it is still important to point out the issues I found. First of all, the paper was not clear 
in the details of the splitting strategy of the dataset so I had to make some assumptions. Additionally, it was not clear 
what loss function was used for training the model. As the dataset only contains raw cuts of the background noise, and 
the researchers opted to mix the speech signal with the background noise so that they have full control over the SNR of the signal,
they used another dataset to mix the speech signal with the background noise. More specifically, they used the CHiME 3 \cite{barker_third_2015} 
and CHiMe 5 \cite{barker18_fifth_2018} datasets. However, only the CHiME 3 development set was used for mixing the speech into the background noise.
CHiME 5 was used for creating a new environment, 'Interferring Speakers', which are samples that contain speech from multiple speakers.
I think using the CHiME 3 dataset and especially only the development set was an odd decision, as there is only roughly 10 hours of data 
from 4 speakers. So a model trained on this dataset may not generalise well to real world scenarios, due to the rich variability of 
speech from different speakers. Nevertheless, we continue with the paper's approach to set up a baseline. Another odd decision was 
in the use of a fixed learning rate scheduler, and the use of a high epoch count without what appears to be no early stopping. This 
will be discussed in more detail in Chapter \ref{chap:acoustic-scene-classification}. We still thank the authors for their work on the dataset 
as it is a promising dataset for the future of HA research. Additionally, given that the dataset 
has the potential to be used for HA devices, there is not any mention on the viability of the models 
when used in a HA device. So a significant aspect of this project is to investigate the feasibility of 
incorporating a DNN model into a HA device. 


\chapter{Experimental Setup}
\section{Datasets}
\label{sec:datasets}
\subsection{HEAR-DS}
HEAR-DS is a dataset created by \citet{Huwel2020HearDS} which is specially tailored for Hearing Aid (HA) research. 
It consists of recordings captured on a dummy head equipped with Pinnae models and three microphones per side: one In-The-Canal (ITC) and two Behind-The-Ear (BTE) microphones (front and rear).
Each sample is recorded in either the left or right channel and decomposed into its respective microphone channels (BTE\_front, BTE\_rear, ITC), available in 48kHz/32bit format.
In the paper, the researchers used only the ITC samples, so for the scope of this project, we will only use the ITC samples as well. 
As the researchers were based in Germany, their recordings were situated in Germany.
The dataset provides 7 environments:
\begin{itemize}
   \item \textbf{Cocktail Party} - Covers situations such as multiple speakers in a noisy environment or 'babble' noise. The paper mentions that this was mimicked by recording them in a university cafeteria or at a senior citizens' meeting.
   \item \textbf{Wind Turbulence} - Concerns the sound that is produced when wind passes through a microphone.
   \item \textbf{In-Traffic} - Samples where traffic noise is dominant such as bus stations or sidewalks. The researchers controlled for this environment to not be similar to the \textit{Wind Turbulence} environment by choosing 'calm' days for the recordings.
   \item \textbf{In-Vehicle} - The sounds produced when seated inside a car. The researchers controlled for this environment to not be similar to the \textit{In-Traffic} or \textit{Wind Turbulence} environments by ensuring the windows were closed. 
   \item \textbf{Quiet Indoors} - Mimic noises produced in a typical household such as washing dishes, clock ticking, etc. The researchers controlled for variability by recording the samples in several flats (rural, city centre, etc).
   \item \textbf{Reveberant} - Samples in highly reverberant environments such as a railway station hall, staircases or a church. 
   \item \textbf{Music} - The data is actually taken from the GTZAN dataset \cite{tzanetakis_musical_2002} and is used to test the model's ability to classify music.
\end{itemize}
Additionally, the researchers artificially created a new environment, \textit{Interfering Speakers}, which are samples that contain speech from multiple speakers. 
This is done by taking speech samples from the CHiME 5 dataset \cite{barker18_fifth_2018}. This environment is supposed to mimic the 
typical conversational speech that occurs in a real-world scenario. We however, decided to use the CHiME 6 dataset which 
supercedes the CHiME 5 dataset \cite{barker18_fifth_2018} by correcting the alignment of audio channels and the researchers 
recommend the use of the CHiME 6 dataset instead of CHiME 5 for any new research so we will use that instead. This could 
be an important factor as the methodology of creating the \textit{Interfering Speakers} environment is by finding 10s 
segments of speech that contain multiple speakers, so if the alignment of audio channels is not correct, the recordings 
may have a delay between the left and right channels which could have potentialy affected the results of the paper. We will discuss the dataset in more detail in section \ref{sec:chime6}.

To prevent the model from overfitting and increase the diversity of the data, the researchers provided multiple REC-SITs (Recording Situations) for each environment.
In the case of the \textit{Music} environment, each REC-SIT represents a different genre of music. This has some potential to cause issues 
and we discuss this in more detail once we aim to reproduce the results of the paper (Chapter \ref{chap:acoustic-scene-classification}). 
As for the artificially created \textit{Interfering Speakers} environment, it is unclear how they define REC-SITs for this environment. 
What I did was to extract the session ID from the CHiME 6 dataset and use that as the REC-SIT, which 
gives us a total of 16 REC-SITs for this environment.

The model presented in the paper assumed a sampling rate of 16KHz, so we resampled the HEAR-DS dataset to 16KHz. Additionally, 
the model expected a log-mel spectrogram of 10s segments, and the HEAR-DS dataset provided longer segments. We therefore 
split the HEAR-DS segments into 10s segments by traversing the dataset and extracting 10s segments. There is potential 
for this to have discontinuities in the beginning and end of the segments, but from the paper it is unclear how this 
was addressed if at all. We went with a naive approach of just cutting the segments.

% TOOD: Make sure to refer to this problem later


\subsection{CHiME3}
As mentioned in the previous chapter, the researchers used the CHiME 3 dataset to mix the speech signal with the background noise. 
CHiME 3 is a dataset published in 2015 for ASR tasks, and was dedicated to improving the performance of mobile devices 
in everyday, noisy environments. The vocabulary of the speech samples is taken from a subset of the Wall Street Journal (WSJ0) corupus \cite{TODO}.
We won't go into the details of the environments, because while not exactly clear from 
the paper, it is mentioned that a development set was used for the mixing. We assume that they used the isolated development set 
recorded in a booth, as it would not make sense otherwise since the other sets contain noisy samples.
The use of the development set for mixing is not ideal, as it is not representative of the real-world data.
In particular, as can be seen from Table \ref{tab:chime3-stats}, it contains 10 hours of data from 4 speakers. This could be a problem in both tasks (ASA and Speech Enhancements), in the former
we could be overfitting to the data due to the Pigeon-hole principle i.e. we have much more background samples than speech samples, 
and in the latter, the enhancement model may not generalise well to real-world scenarios due to the small size of the dataset. 
A better approach would of perhaps been to use the cleanm speech samples 
Nevertheless, we will continue with the paper's approach to set up a baseline. The data was already in 16KHz so we did not need to 
resample it. 
% TODO: Add table here
\subsection{CHiME5/CHiME6}
The CHiME 5 dataset was originally used in the paper to create the \textit{Interfering Speakers} environment for the HEAR-DS dataset.
However, since the CHiME 6 dataset supersedes CHiME 5 with improved audio channel alignment, we opted to use CHiME 6 for our implementation.
CHiME 6 (and CHiME 5) was a dataset created for the Speech Seperation and Recognition Challenge in 2020. It 
focuses on conversational speech in everyday home environments and particular emphasis was placed on 
eliciting a 'dinner party' scenario i.e. a mixture of speech from multiple speakers. It was not 
clear from the paper what sets were used for the creation of the \textit{Interfering Speakers} environment 
however, they did provide a table of number of samples in each environment. From the table, we 
were able to deduce that they must have used the training, development and evaluation sets for the creation of the \textit{Interfering Speakers} environment.
We talk more about the creation of this environment when we discuss the baseline model in Chapter \ref{chap:acoustic-scene-classification}.
From Table \ref{tab:chime6-stats}, we can see that the dataset contains almost 50 hours of data from 48 speakers. 
\label{sec:chime6}
TODO Table

\subsection{TUT Acoustic Scenes 2016}
As mentioned in the previous chapter, I came across better results than the baseline model in the paper by \citet{Huwel2020HearDS} 
so as a means of validation, I decided to use the TUT Acoustic Scenes 2016 dataset \cite{mesaros_tut_2016} to test the performance of the model.
In particular, I will comparing the results by \citet{schindler_multi-temporal_2018}. The paper presents a multi-temporal approach to ASA, and 
the authors have used the development set of the TUT Acoustic Scenes 2016 dataset for their experiments. It is worth stressing 
that this dataset contains background noise only, and the paper does not evaluate it with speech, so when we use it to validate 
the model, we will maintain the same approach as the paper.

\section{Baseline Models}
\subsection{Acoustic Scene Classification}
For the ASA task, the baseline model is the CNN model presented in the paper by \citet{Huwel2020HearDS}. 
The researchers investigated the effect of differentt model parameters, where the first 
layer has \textbf{CNN\_1} output channels  and the second layer doubles the number of output channels (\textbf{CNN\_2} = 2$\times$CNN\_1).
While the researchers vary the number of output channels in intervals of 4, thereby training 7 different models, we will 
only train 3 models, as we're mostly interested in seeing if we're getting similar results to the paper.
Table \ref{tab:cnn-model-params} shows the parameters of the models we will be training. 
So the sequence of operations for the model is as follows:
\begin{enumerate}
   \item First CNN block:
      \begin{itemize}
         \item 2D Convolution (\textbf{CNN\_1} channels, 7x7 kernel, 1x1 stride, 3x3 padding)
         \item Batch Normalisation
         \item ReLU
         \item MaxPool (5x5 kernel and stride)
         \item Dropout (0.3)
      \end{itemize}
   \item Second CNN block:
      \begin{itemize}
         \item 2D Convolution (\textbf{CNN\_2} channels, 7x7 kernel, 1x1 stride, 3x3 padding)
         \item Batch Normalisation
         \item ReLU
         \item MaxPool (4x100 kernel and stride)
         \item Dropout (0.3)
      \end{itemize}
   \item Fully Connected block:
      \begin{itemize}
         \item Flatten
         \item ReLU
         \item Dropout (0.3)
         \item Linear layer (10 output classes)
      \end{itemize}
\end{enumerate}
This is better visualised in Figure \ref{fig:cnn-model-architecture}.

\begin{table}[h]
   \centering
   \begin{tabular}{|c|c|c|c|}
      \hline
      Model & CNN\_1 & CNN\_2 & FC \\
      \hline
      net-8 & 8 & 16 & 25 \\
      net-20 & 20 & 40 & 63 \\
      net-32 & 32 & 64 & 100 \\
      \hline
   \end{tabular}
   \caption{Model parameters for the net-X models.}
   \label{tab:cnn-model-params}
\end{table}



\section{Hypotheses}

\begin{enumerate}
   \item \textbf{Using Fixed Learning Rate Stochastic Gradient Descent (SGD) will result in a longer training time than using Adam Optimiser.} The paper by \citet{Huwel2020HearDS} used the fixed learning rate SGD approach.
   I hypothesise that using Adam Optimiser will result in a shorter training time.
   \item \textbf{Data Augmentation will result in a higher accuracy of the model.} It is unclear from the paper whether the authors used data augmentation...
   \item \textbf{Using an LSTM model will result in a higher accuracy of the model.}
\end{enumerate}


\chapter{Acoustic Scene Classification Experiments}
\label{chap:acoustic-scene-classification}
\section{Data Preparation}
TODO
\subsection{Fixed Learning Rate SGD Approach}
TODO
\subsection{Adam Optimiser Approach}
TODO
\subsection{LSTM Model}
TODO

\chapter{Speech Enhancement Experiments}
\section{Data Preparation}
TODO
\subsection{CNN Model}
TODO

\chapter{Conclusions}
\section{Discussion}
TODO
\section{Future Work}
TODO









% \chapter{Conclusions}

% \section{Final Reminder}

% The body of your dissertation, before the references and any appendices,
% \emph{must} finish by page~40. The introduction, after preliminary material,
% should have started on page~1.

% You may not change the dissertation format (e.g., reduce the font size, change
% the margins, or reduce the line spacing from the default single spacing). Be
% careful if you copy-paste packages into your document preamble from elsewhere.
% Some \LaTeX{} packages, such as \texttt{fullpage} or \texttt{savetrees}, change
% the margins of your document. Do not include them!

% Over-length or incorrectly-formatted dissertations will not be accepted and you
% would have to modify your dissertation and resubmit. You cannot assume we will
% check your submission before the final deadline and if it requires resubmission
% after the deadline to conform to the page and style requirements you will be
% subject to the usual late penalties based on your final submission time.

\bibliography{mybibfile}


% You may delete everything from \appendix up to \end{document} if you don't need it.
% \appendix

% \chapter{First appendix}

% \section{First section}

% Any appendices, including any required ethics information, should be included
% after the references.

% Markers do not have to consider appendices. Make sure that your contributions
% are made clear in the main body of the dissertation (within the page limit).

% \chapter{Participants' information sheet}

% If you had human participants, include key information that they were given in
% an appendix, and point to it from the ethics declaration.

% \chapter{Participants' consent form}

% If you had human participants, include information about how consent was
% gathered in an appendix, and point to it from the ethics declaration.
% This information is often a copy of a consent form.


\end{document}
