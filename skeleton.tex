% UG project example file, February 2024
%
%   Added the "online" option for equal margins, February 2024 [Hiroshi Shimodaira, Iain Murray]
%   A minor change in citation, September 2023 [Hiroshi Shimodaira]
%
% Do not change the first two lines of code, except you may delete "logo," if causing problems.
% Understand any problems and seek approval before assuming it's ok to remove ugcheck.
\documentclass[logo,bsc,singlespacing,parskip,online]{infthesis}
\usepackage{ugcheck}


% Include any packages you need below, but don't include any that change the page
% layout or style of the dissertation. By including the ugcheck package above,
% you should catch most accidental changes of page layout though.

\usepackage{microtype} % recommended, but you can remove if it causes problems
\usepackage[square,numbers]{natbib} % recommended for citations
\usepackage{graphicx}
\usepackage{amssymb}
\graphicspath{ {./images/}}
\bibliographystyle{unsrtnat}

\begin{document}
\begin{preliminary}

% \title{Investigating Machine Learning Techniques for Hearing Aids}
\title{Towards a State-Dependent Model for Speech Enhancement in Hearing Aids}

\author{Nikodem Bieniek}

%\course{Artificial Intelligence and Computer Science}
\course{Master of Informatics} % MInf students

\project{MInf Project (Part 1) Report}  % 4th year MInf students
%\project{MInf Project (Part 2) Report}  % 5th year MInf students


\date{\today}

\abstract{
   This dissertation explores the intersection of Machine Learning, Automatic Speech Recognition (ASR) and Hearing Aids (HA) 
   through the lens of Acoustic Scene Analysis (ASA) and Speech Enhancement (SE).
   The primary objective of this dissertation is to develop machine learning models capable of prioritising 
   speech in noisy environments whilst remaining computationally feasible for implementation in hearing aids.
   The vision is to chain the ASA and SE models together to form a state-dependent model, in essence,
   a heuristic approach to SE that is tailored to the environment.

   Accordingly, each proposed model is evaluated not only in terms of performance but also on its feasibility for real-time deployment, as measured by metrics such as Floating Point Operations Per Second (FLOPS) and the number of model parameters.
   
   A growing trend in the market is the incorporation of deep neural networks (DNNs) into hearing aids.
   Evidence from state of the art manufacturers suggests that a combination of ASA and SE techniques are employed,
   further motivating the approach adopted in this project.
   % As such, this project will aim to mimic this behaviour by 
   % first classifying the environment using a machine learning model, and then applying
   % heuristic speech enhancement techniques to prioritise speech in particular environments.
   
   Experiments are conducted primarily on a novel dataset called HEAR-DS.
   As a baseline, the Convolutional Neural Network (CNN) ASA model presented by the authors of the dataset is utilised.
   We will demonstrate improvements in both the training procedure and the generalisation capability 
   of the baseline CNN through the implementation of data augmentation strategies and the adoption of an alternative optimiser.
   
   Furthermore, the dataset is extended by demonstrating a proof-of-concept of a speech enhancement model 
   using a shallow CNN model that works in the frequency-time domain. We evaluate the performance of the model 
   using artificial intelligibility and quality metrics such as STOI and PESQ. While the performance is not on par 
   with the state-of-the-art, it most certainly motivates further research in the next part of the dissertation.
}

\maketitle

\newenvironment{ethics}
   {\begin{frontenv}{Research Ethics Approval}{\LARGE}}
   {\end{frontenv}\newpage}

\begin{ethics}
This project was planned in accordance with the Informatics Research
Ethics policy. It did not involve any aspects that required approval
from the Informatics Research Ethics committee.

\standarddeclaration
\end{ethics}


\begin{acknowledgements}
First and foremost, I would like to solemly thank my supervisor, Hao Tang for 
his enthusiasm in guiding me through this self-proposed dissertation. I am eternally grateful for his wisdom, guidance, and patience throughout the project.

I would like to also thank my friends, for their support and encouragement throughout the project. 

And to my parents, without whose sacrifices I would not have been able to pursue this degree, let alone this project.
\end{acknowledgements}


\tableofcontents
\end{preliminary}


\chapter{Introduction}
\section{Motivations}
Hearing loss is a prevalent condition affecting as much as 
430 million people - or 1 in 18 people. This is expected 
to rise to 1 in 10 by 2050 \cite{WHO2024deafness}.
The most common treatment for hearing loss is the 
provision of hearing technology - such as Hearing Aids (HAs) or cochlear implants.
There are many types of HAs, but the most common
type is the behind-the-ear (BTE) hearing aid \cite{Kochkin2010MarkeTrak8}.
However, HA users often report that they struggle to hear speech
in noisy environments. For example, in a study by Kochkin \cite{Kochkin2010MarkeTrak8},
42\% of HA users reported that wind noise was a significant issue for them.
This project aims to evaluate the effectiveness of machine learning algorithms
in prioritising the speech in various environments (such as windy environments).

Modern hearing aids now apply a wide range of techniques to achieve 
better speech prioritisation. For wind noise reduction, this can be achieved 
from mechanical solutions - product design to covers that reduce wind noise - to 
signal processing techniques to compensate for mechanical limitations.
However, current techniques are still not perfect as shown by the study from Kochkin.

Wind noise reduction and indeed, noise reduction in general, is a challenging problem
when paired with speech. This is because you have 
to strike a balance between reducing background noise and
preserving speech. 
Korhen's paper \cite{Korhonen2021WindNoise} outlines 
various techniques that could be used to reduce the wind noise in hearing aids -
from modulation-based noise reduction algorithms (Wiener filtering),
adaptive filtering algorithms, to machine learning techniques.
The paper mentions that the the proposed ML technique:
Long Short-Term Memory (LSTM) neural networks provided
modest improvements in wind noise reduction, however, it did highlight
that ML techniques may still have utility through further research 
and careful algorithmic choices. 

This project aims to investigate the effectiveness of machine learning techniques 
in prioritising speech in noisy environments. The idea is to 
first train a deep neural network (DNN) to perform acoustic scene analysis (ASA) to classify the environment.
Afterwards, a speech enhancement model will be trained to enhance the speech in the environment.
From now on, we will refer to the chaining of the ASA and SE models together as a state-dependent model
as inspired by \citet{katagiri_handbook_2000}'s categorisation of SE techniques (see \ref{sec:background:se} for more details).

In this project, we will be using a novel dataset proposed by \citet{Huwel2020HearDS}
This dataset (called HEAR-DS) is unique because it is specially tailored for HA signal processing and contains
various environments. Normally, voice activity detection (VAD) would be 
used to detect speech, however, we will take inspiration from \citet{Huwel2020HearDS}
by mixing the samples with speech and label them as so. 
This can be used to implicitly train the machine learning model to classify the environment 
and whether speech is present. Additionally, the paper presents an 
elementary example of how the dataset can be used: to classify the environment -
it showcases the use of a convolutional neural network (CNN) to classify the environment.
This project will be extending the paper by looking at how HEAR-DS can be used 
to additionally train a speech enhancement model and how the training and 
generalisation capabilities of the model can be improved.

The project will also be mindful in its algorithmic choices - as the computational
power required in HA is limited. It is difficult to pinpoint the exact computational power of a HA
due to the proprietary nature of the devices. In August 2024,
Phonak (Sonova Holding AG) released a new HA which is their first AI 
equipped HA. The device is said to be capable of handling 7,700 Million 
Operations Per Second to accommmodate its neural network with 4.5 million parameters \cite{Hasemann2024PhonakSphere}.  Contrast this with a paper 
from 2021 investigating techniques in VAD for hearing aids 
quotes that it `rarely exceeds 5 million instructions per second (MIPS)` \cite{Gomez2021MIPS}
Moreover, Apple's release of a FDA approved hearing aid in which was previously a mainstream earphone wearable,
Apple Airpods, also shows that there's more interest in this area.
So suffice to say, the computational power of hearing aids is accelerating 
and is most likely going to continue to grow given the increasing demand 
for HAs.

Delay constraints also play a critical role in HA performance, as real-time speech perception is essential for user satisfaction.
Prior research suggests that a delay of up to 30 milliseconds (ms), and ideally less than 20ms, the user
is unaffected by the delay \cite{Stone2002Delays}. Therefore, the project will be mindful in its algorithmic choices to ensure that the model
is computationally efficient and feasible for real-time deployment.

The main evaluation metrics we will consider in this part of the dissertation 
is short-time objective intelligibility (STOI) and perceptual evaluation of speech quality (PESQ). 
These are both artificial metrics and give an indication of the quality of the speech enhancement model. 
Their limitations are discussed in chapter \ref{chap:discussion}, with subsequent sections outlining strategies to address these shortcomings.
Additionally, informed by current state-of-the-art HA configurations, the project will also assess model complexity by analysing the number of weights utilised in our designs.

The project is predominantly aimed at the hearing aid industry. If successful, the project
could advance the state-of-the-art techniques in hearing aids.
Which in turn could improve the quality of life for hearing aid users.
A secondary goal is to make hearing aid research more accessible to the 
computer science community.
The project also hopes to benefit other fields that deal with audio signal processing,
such as mainstream wearables like headphones or microphones.


\section{Structure}
Going forward, the dissertation will be structured as follows:
\begin{itemize}  
   \item \textbf{Chapter \ref{ch:background}} - This chapter will give a high level overview of hearing loss, the auditory system, and the speech processing concepts needed to understand the project.
   We briefly also cover the two techniques that will be used in this project: Acoustic Scene Analysis (ASA) and Speech Enhancement
   and some of the related work done in the field. Lastly, I mention the criticism of the paper by \citet{Huwel2020HearDS} 
   that I will be exploring in this project.
   \item \textbf{Chapter \ref{ch:methodology}} - lorem ipsum
   TODO
   % \item \label{ch:Methodology} - This chapter will give a high level overview of the 
   % methodology used in this project. It will cover the datasets that will be used, the baseline models that will be used, and the hypotheses that will be tested.
   % \item \label{ch:Results} - This chapter will cover the results of the experiments.
   % \item \label{ch:Discussion} - This chapter will discuss the results of the experiments and the implications of the results.
   % \item \textbf{Experimental Setup} - This chapter will cover the datasets that will be used in this project, the baseline models that will be used, and the hypotheses that will be tested.
   % \item We then move onto the results of the experiments.
   % \begin{itemize}
   %    \item \textbf{Acoustic Scene Classification} - This section will cover the results of the experiments pertaining to the ASA task.
   %    \item \textbf{Speech Enhancement} - This section will cover the results of the experiments pertaining to the speech enhancement task.
   % \end{itemize}
   % \item \textbf{Discussion/Conclusion} - This chapter will discuss the results of the experiments and the implications of the results.
\end{itemize}
 

\chapter{Background}
\label{ch:background}

\section{Hearing Loss}
\subsection{Auditory System}
For sound to be registered by humans, it has to travel through the ear to transform them into
what is known as a neural impulse which is then trasmitted to the brain. We will 
give a high level overview of the process which was collated by the phonetics textbook from \citet{Wayland2018Phonetics},
 but for further reading, refer to the textbook. 
 This process of sound to be registered by humans, is all done in what's known as the Auditory System.
 Figure \ref{fig:ear} shows a birds eye view of the auditory system, and it can be split into three segments
 that we will explore below.

\begin{figure}[h]
   \centering
   \includegraphics[width=0.5\textwidth]{wayland-ear}
   \caption{The external, middle and the inner ear from \citet{Wayland2018Phonetics}}
   \label{fig:ear}
\end{figure}

\subsubsection{The Outer Ear}
The outer ear (sometimes referred to as the External Ear) is responsible for channeling sound waves to the tympanic membrane (ear drum). Firstly,
sound waves reach the Pinna which funnels it through the auditory canal, and at the end of the canal 
is the ear drum, at which point the sound waves collide with the ear drum which causes the ear drum to vibrate. 
The vibrations are passed along to the middle ear.

\subsubsection{The Middle Ear}
Zooming into the middle ear (Figure \ref{fig:middle-ear}), there are three bony structures: the Malleus, Incus, and Stapes,
and together they make up a lever system. 
It is worth pointing out that when transmitting the energy from the ear drum to the middle ear, there is bound to be some energy 
loss \footnote{Depending on the frequency, this can be as high as 40\%}. The lever system's mechanical advantage 
allows for the sound energy to be amplified so the loss is compensated. 
The Stapes also has an additional function besides passing over the sound vibrations to the inner ear. The Stapes 
is connected by a muscle, the Stapedius muscle, and in response to loud noises, it temporarily increases the stifness
of the bones in the middle ear, which temporarily prevents the acoustic energy to be amplified. This 
protects the inner ear from loud noises.

\begin{figure}[h]
   \centering
   \includegraphics[width=0.5\textwidth]{wayland-middle-ear.png}
   \caption{The components of the middle ear from \citet{Wayland2018Phonetics}}
   \label{fig:middle-ear}
\end{figure}

\subsubsection{The Inner Ear}
One of the parts of the inner ear, is the Cohlea, and it is what gives humans the ability to hear sounds.
The cohlea is a bony structure that resemblers that of a snail shell \footnote{'Cohlea' in Latin translates to snail shell!}.
If the cohlea was unrolled its length would be about 3.5cm, and it is subdivided into various parts (see Figure \ref{fig:cohlear}).
For our purposes, it suffices to know that in the Basilar membrane are a collection of cells, called 
the Organ of Corti, which contain hair cells, and that different hair cells register different frequencies.
Figure \ref{fig:corti} shows the frequency responses shows the various areas of the corti 
and its response to different frequencies. The Organ of Corti is linked to the audtiory nerves and 
so the oscillations that pass through it get coverted to neural impulses and transmitted to the brain.


\begin{figure}[h]
   \centering
   \includegraphics[width=0.5\textwidth]{wayland-cohlear.png}
   \caption{'Unrolled' cohlear from \citet{Wayland2018Phonetics}}
   \label{fig:cohlear}
\end{figure}

\begin{figure}[h]
   \centering
   \includegraphics[width=0.5\textwidth]{wayland-corti.png}
   \caption{The Organ of Corti and the frequency responses. From \citet{Wayland2018Phonetics}}
   \label{fig:corti}
\end{figure}

\newpage

% \subsection{Congenital Hearing Loss}
% TODO
% \subsection{Exposure to Loud Noises}
% TODO
% \subsection{Ageing}
% TODO
\subsection{Hair Cell Loss and Hearing Loss}
As we have seen, the hair cells play a major role in allowing humans to register sound. Unfortunately,
it is the major cause of hearing loss since those hair cells are suspectible to damage.
The cause of hair cell loss is complex, since it could be due to many factors such as 
genetic abnormalities (congenital hearing loss), infection, diseases or extrinistic factors such as exposure to loud noises.
It can also occur due to aging, which could be due to the reasons mentioned above, but also 
could be triggered due to age related hearing loss genes. To make matters worse, hair cell loss is unrecoverable in humans \cite{Furness2015HairCell}.
Hearing loss is a spectrum, and varies from mild (26-40dB loss), moderate (41-60dB loss), severe (61-80dB loss) to profound($>80$dB loss) \cite{Nieman2020HearingLoss}.
Moreover, hearing loss may occur in one ear (unilateral) or in both ears (binaural). 

\section{Hearing Loss Treatment}

\subsection{History of Hearing Technology}
TODO: Will see how much pages I have left after doing other chapters.

Users were fitted with what would have been considered a HA in this day and age as early as the 18th century.


\subsection{Hearing Technology}
As mentioned in the Introduction, the most common treatment for hearing loss 
is the provision of hearing technology. There are two common approaches 
to fitting of hearing technology: the equipment of a Hearing Aid (HA) 
or the fitting of a Cohlear Implant. There are similarities in the 
two, and both can be used to fit users that are experiencing profound hearing loss, 
though typically cohlear implants are only considered if the user is experiencing 
a severe hearing loss or further. While we 
focus on the HA, the concepts we explore can be applied to the CI.
The components of a HA vary widely, but at the very least, 
it contains a microphone, amplifier, receiver, and battery \cite{schuster-bruce_conventional_2025}.
A microphone is a device that converts acoustic sound waves into electrical signals. 
The electrical signals are then amplified by the amplifier, and the receiver converts the electrical 
signals back into the acoustic waves. The battery is used to power the device. 
Depending on the user, they may either be fitted with a hearing aid on each 
ear (binaural) or a single hearing aid on one ear (unilateral). 
Typically, the hearing aid contains multiple microphones, and the microphones 
are placed in different locations on the device. As you will see when we discuss 
the dataset we will be using, the samples are labelled with which microphone 
is being used. 


\subsection{Hearing Technology Now}
Hearing aids are increasingly becoming more sophisticated and are starting to incorporate more powerful 
algorithms to improve the quality of life for hearing aid users. From Apple's FDA approved hearing aid in 
their Airpods, to big traditional players like Phonak, Oticon who are starting to incorporate 
DNN and AI into their devices. There is also a trend of rechargable hearing aids (Lithium Ion Batteries), and while 
it has a practical aspect i.e. the user does not need to change the batteries, it's also 
specifically used because of the ability to handle higher peak power consumption (which allows for powerful models to be used such as DNNs).
Whereas non-rechargable hearing aids are restricted to what's allowed by the battery. There is however, a trade-off,
as the battery life of the device is sacrificed. Battery powered hearing aids 
are most always using Zinc-air batteries \cite{sparkes_study_1997} \cite{mir_evaluation_2023} 
and the most recent study by \citet{thomas_zincair_2024} shows that the average battery life 
under high power conditions 50-80 hours. Contrast this with the average battery life of 
rechargable hearing aids which is typically in the region of 15-30 hours. In the case of 
Phonak's Speech Enhancer that kicks in when the user is in a loud environment, the 
maximum battery life is under 6 hours. This goes to show that 
using DNNs in HA can require careful design considerations but the potential benefits 
are worth it.


Due to the proprietary nature of the devices, it is unclear what sort of 
algorithms are used in the devices. Though, from Phonak's white paper on Spheric Speech Clarity \cite{Hasemann2024PhonakSphere}
we can deduce that the devices use a combination of ASA and speech enhancement techniques.
From the paper, before signal processing begins, the sound is fed into Autosense OS which 
performs 'scene classification' so in the context of this project, this can be 
considered as an acoustic scene analysis (ASA) algorithm. More specifically, if 
the signal is classified as 'Speech in Loud Noise', then the signal is fed into a 
deep neural network which outputs a mask that seperates the speech signal from the background noise 
which is then applied to the signal. In the context of this project, we will aim to do something similar 
when we train a speech enhancement model on HEAR-DS. 



% It is enough 
% to know for now that the devices will contain multiple microphones, and 
% a dedicated chip which does further processing on the sound depending 
% on the environment the user is faced with.
% It should be pointed out that there is an active area 
% of research into curing congenitial hearing loss - individuals who have been diagnosed with 
% a hearing loss since birth.
% \subsubsection{Hearing Aids}
% Figure ... shows a typical hearing aid, and Figure ... shows the hearing aid labelled.
% TODO
% Microphone info, the reason for multiple mics etc
% There is a surge into rechargable hearing aids.
% Smaller model = higher battery life 

% \subsection{New Approaches to Treatment}
% \subsubsection{Gene-Based Therapy}
% Genitic mutations account for 70-75\% of congenital hearing loss, and with the advent 
% of gene-based therapy, it is not surprising to see that this avenue is explored with hearing loss. 
% However, besides it being an active area of research, it will not be applicable 
% to individuals that have 

\section{Speech Processing Techniques}
\subsection{Digitisation}
To be able to perform speech processing on a device containing a chip/processor such as a HA, requires us 
to have some sort of digital representation of the sound wave. A waveform is just that, a digital representation 
of the sound we hear. A microphone is a device that converts those sound waves into a representation of the 
waveform. Conceptually, this is done by measuring the relative air pressure at different points in time, and 
this is represented as a waveform or a time series of amplitudes. Digital devices can't have infinite precision, so the signal produced by the sound will 
be divided into discrete samples, and the rate at which this occurs is known as the sampling rate. Additionally,
the precision of the amplitude of the signal will be quantized into discrete numbers, and the precision is 
dictated by the bit depth (or the quantization rate).

The selection of these two parameters is determined by the nature of the speech task and the Nyquist-Shannon Sampling Theorem.
This theorem asserts that the sampling rate should be at least twice the highest frequency present in the signal. 
That is, for a signal containing frequencies up to 8KHz, the minimum required sampling rate would be $f_s \ge 16$KHz.
Otherwise, aliasing will occur, which is the phenomenon where high frequency components could be misinterpreted as lower frequencies,
which could be undesirable in both tasks we are trying to perform. Typically, datasets will have a sampling rate 
of 44KHz, and commonly for ASA tasks, a sampling rate of 16KHz is acceptable. 
Regarding the bit depth, it is selected based on the dynamic range required to capture speech signal variations. 
Commonly, 16-bit depth is used, which is capable of representing an amplitude range of approximately 96dB, thereby ensuring sufficient resolution in amplitude representation.

\subsection{Engineered Acoustic Features}
The prevalance of engineering acoustic features in previous work relating to my project and speech processing in general,
warrants a high level overview of this concept to better understand its usage. 
Given some signal $x(t)$, an engineered feature $f(x(t))$ is a function that transforms the signal in some way.
In this project we will be feeding the models the log mel spectrogram of the signal. To obtain the log mel spectrogram, 
we first need to convert the signal (waveform) into the frequency domain (spectrogram).
The process of converting the signal into the frequency domain is done by utilising 
the Fourier Transform. We will technically be using the Fast Fourier Transform (FFT) 
due to efficiency, and the main hyperparameters for the FFT are the window length $w$ 
and the hop length $h$. The window length is the length of the window that is used to 
split the signal into frames, and the hop length is the number of samples between the 
start of each frame. Additionally, there is a choice of a window function and we will 
be using the most common one, the Hann window. Once we apply the FFT, we will be 
left with a complex valued vector of length $w$ and the magnitude of this vector 
gives us the amplitude of the signal at different frequencies. 
We can then apply a Mel Filterbank - a filterbank that is used to mimic the human auditory system. 
Afterwards, we take the logarithm of the filterbank outputs and we are left with the log mel spectrogram 
which mimics the human perception of loudness. Diagram \ref{fig:log-mel-spectrogram} shows the process to make it more clear.

The reason it is worthwhile to use an engineered feature is for many reasons, but one of the main ones is that 
it is a compact representation of the signal which reduces the amount of memory and computational resources required 
to process the signal which is especially important for embedded devices such as HAs. 
Additionally, the log mel spectrogram enhances the distinction between speech and background noise, which 
may make a model more effective at ASA and speech enhancement tasks.
\section{Related Work}
As this project will chain two algorithms—Acoustic Speech Analysis (ASA) and Speech Enhancement, 
to form a state-dependent model, we will cover each task in separate sections.

\subsection{Acoustic Scene Analysis (ASA)}
Acoustic Scene Analysis (ASA) refers to the task of classifying an environment based on an audio stream. 
% Mathematically, ASA can be formalised as follows:

% Consider an audio signal \(x \in \mathbb{R}^{T}\), where \(T\) denotes the number of samples in the signal, and let \(\mathcal{E} = \{e_1, e_2, \ldots, e_N\}\) represent a set of \(N\) distinct environments. ASA is concerned with determining a mapping 
% \[
% f: \mathbb{R}^{T} \rightarrow \mathcal{E},
% \]
% which assigns each audio signal to one of the environments in \(\mathcal{E}\). In this project, \(f\) is implemented as a neural network. Therefore, training the model requires a dataset containing audio signals along with their corresponding environmental labels and relevant recording metadata.

We will be using HEAR-DS dataset introduced by \citet{Huwel2020HearDS}. This dataset was developed in response to the inadequacies of existing acoustic scene classification databases for HA processing. A more detailed description of the dataset is provided in Chapter \ref{sec:datasets}.
The original paper on the HEAR-DS dataset illustrates its applicability by employing a Convolutional Neural Network (CNN) for environment classification. In Chapter \ref{chap:acoustic-scene-classification}, an attempt is made to reproduce the results presented in that study. The challenges encountered during this replication process are discussed comprehensively in the chapter. Additionally, to validate the model implementation, the same parameters were applied to the DCASE 2017 Acoustic Scenes Challenge dataset \cite{DCASE2017challenge}. In particular, a comparison is drawn between the model proposed by \citet{schindler_multi-temporal_2018} and the CNN model used in the HEAR-DS paper, with the findings detailed in Section \ref{sec:DCASE-results}.
The two papers mentioned above utilise engineered features, and more specifically, the log mel spectrogram. This 
approach in some literature is called operating on the frequency domain. It is worth pointing out 
that there is research into doing ASA on the time domain i.e. operating on the waveform directly, 
such as the paper by \cite{dai_very_2016} - \cite{kumar_end_2020}. While not explored in this part, 
it is something I will look into exploring in part 2 of this dissertation as there is 
a compelling reason to possibly consider the time domain approach. Namely, 
the overhead of the Fourier Transform and the Mel Filterbank in a hearing aid 
is potentially too high, and so working directly with the waveform may be a more feasible approach.

% TODO: VERY DEEP CONVOLUTIONAL NEURAL NETWORKS FOR RAW WAVEFORMS


% But it is enough to know that the dataset is classified in an environment of which it is subcategorised as 
% either containing speech only, background only or speech \& background (both). To prevent machine learning 
% models from overfitting and to increase the diversity of situations, each environment enforces a minimum of three recording situations (REC-SITs) i.e.
% there must be at least three different locations for the recordings. The researchers presented the applicability 
% of this dataset by performing a series of classification experiments using various Convolutional Neural Networks (CNN) of differing sizes.
% This is where the paper could be more clear on the design decisions of the CNN, esepcially because the accuracy of the classification of the models 
% can vary as high as 20\%. This project hopes to reproduce the results, and explain the high variability in the accuracy and how those can be circumvented,
% because the high variability suggests that the random initialisation of weights during training is the primary factor influencing the model performance.
% [To Hao: This is what I understood from your interpretation in the discussion of this paper last Friday, did I understand you correctly?].
% For the recordings labeled as being in environments containing both speech and background noise, the researchers decided against recording the samples in situ.
% Instead they took the speech data from CHiME 

\subsection{Speech Enhancement}
On the other hand, Speech Enhancement aims to improve the perceptual quality of a speech signal that 
has been degraded by an additive noise \cite{loizou_speech_2007}. 
This enhancement is crucial in applications where speech intelligibility is essential, such as telecommunications, hearing aids, and speech recognition systems. Noise sources may include environmental disturbances (e.g., wind noise) as well as interference from multiple speakers (e.g., babble noise).
% Mathematically, let a noisy speech signal be represented by \(x \in \mathbb{R}^{T}\) and the corresponding clean speech signal by \(y \in \mathbb{R}^{T}\), where \(T\) denotes the total number of samples.
% Speech enhancement is therefore concerned with determining a mapping 
% \[
% f: \mathbb{R}^{T} \to \mathbb{R}^{T},
% \]
% which takes a noisy speech signal \(x\) and produces an enhanced version \(f(x)\) that is as close as possible to the clean speech signal \(y\) i.e.
% \[
% f(x) \approx y.
% \]
There are many different approaches to speech enhancement, from more traditional methods such as spectral subtraction, or Wiener filtering
to more modern methods that incorporate neural-networks. In this dissertation, we will be using a neural-network based approach.
But even within the neural-network based approach, there are different techniques that can be used to enhance the speech signal. 
Neural network-based speech enhancement algorithms can be categorised into four primary types, as described in \cite{katagiri_handbook_2000}:
\begin{itemize}
   \item \textbf{Time-Domain Filtering} - Trains a neural network with noisy inputs (background + speech) and clean targets (speech only). 
   This involves working with the waveform directly, however it is not without its challenges which is still an active area of research \cite{saleem_time_2024}.
   \item \textbf{Transform-Domain Filtering} - Trains a neural network with noisy inputs (background + speech) and clean targets (speech only). 
   The differnece between this and the Time-Domain Filtering approach is that the input is transformed into the frequency domain 
   (whether that be a Fourier Transform, or a Mel Spectrogram, Mel-Frequency Cepstral Coefficients (MFCCs), LPC, etc). The advantage of this 
   is that the dimensionality of the input is usually lower, and the representation is more robust to seperation of the speech and background noise.
   This is also a popular approach in the speech enhancement literature \cite{hou_local_2023}.
   \item \textbf{State-Dependent Model Switching} - The two mentioned approaches assumes that the speech signal and noise source are stationary. 
   This is too strong of an assumption, and so state-dependent model switching is a paradigm that has been explored. The main idea is 
   to have a class of models, each specialising in handling a different type of noise and to switch between them based on the 
   characteristics of the input signal. This in a way is the chaining of ASA and Speech Enhancement, as the ASA model will 
   first classify the input signal into a state, and then the appropriate speech enhancement model will be selected and applied to the signal.
   \item \textbf{Online Iterative Methods} - The focus is on incorporating adaptive techniques that do not rely on pre-existing 
   training data. Key approaches include adaptive predictors, dual EKF algorithms, and noise-regularised adaptive filtering.
\end{itemize}
It is no accident I talked about Engineering Acoustic Features in the previous section, as this dissertation 
will be focusing on the Transform-Domain Filtering approach. One of the primary objects of part two of this dissertation will be 
investigating how ASA and Speech Enhancement can be integrated to form a state-dependent model.
Though, exploring the time domain approach is something I will be looking into for the second part of this dissertation 
due to the potential overhead reduction of the time domain approach as mentioned in the previous section.

\section{Criticism of Previous Work}
During the course of this project, I found out that the paper by \citet{Huwel2020HearDS} had some issues.
Granted, the paper was more of an exploration of the applicability of the dataset rather than a thorough analysis of the 
performance of the model, it is still important to point out the issues I found. First of all, the paper was not clear 
in the details of the splitting strategy of the dataset so I had to make some assumptions. Additionally, it was not clear 
what loss function was used for training the model. As the dataset only contains raw cuts of the background noise, and 
the researchers opted to mix the speech signal with the background noise so that they have full control over the SNR of the signal,
they used another dataset to mix the speech signal with the background noise. More specifically, they used the CHiME 3 \cite{barker_third_2015} 
and CHiMe 5 \cite{barker18_fifth_2018} datasets. However, only the CHiME 3 development set was used for mixing the speech into the background noise.
CHiME 5 was used for creating a new environment, 'Interferring Speakers', which are samples that contain speech from multiple speakers.
I think using the CHiME 3 dataset and especially only the development set was an odd decision, as there is only roughly 10 hours of data 
from 4 speakers. So a model trained on this dataset may not generalise well to real world scenarios, due to the rich variability of 
speech from different speakers. Nevertheless, we continue with the paper's approach to set up a baseline. Another odd decision was 
in the use of a fixed learning rate scheduler, and the use of a high epoch count without what appears to be no early stopping. This 
will be discussed in more detail in Chapter \ref{chap:acoustic-scene-classification}. We still thank the authors for their work on the dataset 
as it is a promising dataset for the future of HA research. Additionally, given that the dataset 
has the potential to be used for HA devices, there is not any mention on the viability of the models 
when used in a HA device. So a significant aspect of this project is to investigate the feasibility of 
incorporating a DNN model into a HA device. 


\chapter{Methodology}

% The model presented in the paper assumed a sampling rate of 16KHz, so we resampled the HEAR-DS dataset to 16KHz. Additionally, 
% the model expected a log-mel spectrogram of 10s segments, and the HEAR-DS dataset provided longer segments. We therefore 
% split the HEAR-DS segments into 10s segments by traversing the dataset and extracting 10s segments. There is potential 
% for this to have discontinuities in the beginning and end of the segments, but from the paper it is unclear how this 
% was addressed if at all. We went with a naive approach of just cutting the segments.


\section{Acoustic Scene Analysis}
\subsection{Model}
\begin{table}[h]
   \centering
   \begin{tabular}{|c|c|c|c|}
      \hline
      Model & $CNN_1$ & $CNN_2$ & $FC$ \\
      \hline
      net-8 & 8 & 16 & 25 \\
      net-20 & 20 & 40 & 63 \\
      net-32 & 32 & 64 & 100 \\
      \hline
   \end{tabular}
   \caption{Model parameters for the net-X models.}
   \label{tab:cnn-model-params}
\end{table}
As our baseline model, we will be using the CNN model presented in the paper by \citet{Huwel2020HearDS}. 
The model has three hyperparameters: the number of output channels for the first and second convolutional layers ($CNN_1$ and $CNN_2$),
and the number of neurons in the fully connected layer ($FC$). The researchers investigated the effect of different model parameters, 
and in the paper, they train the dataset on 7 different models, where they vary the number of output channels in intervals of 4, thereby training 7 different models.
Due to the time constraints, we will be training 3 models, as we're mostly interested in seeing if we're getting similar results to the paper.
Table \ref{tab:cnn-model-params} shows the parameters of the models we will be training.

The model architecture is shown in Figure \ref{fig:cnn-model-architecture}. As 
mentioned earlier the models operate on the frequency domain, and 
we stuck with the paper's approach of using a Mel Spectrogram as the 
frequency domain representation for reasons mentioned in the background section. 
As per the paper, all the convulaotional layers use a $7 \times 7$ kernel
with a stride of $1 \times 1$ and padding of $3 \times 3$. After the convoluational operation, 
batch normalisation is applied which can help with the stability of forward propagation 
and can act as a regulariser which can help prevent overfitting \cite{prince2023understanding}.
The choice of activation function is a Rectified Linear Unit (ReLU) which is computionally 
efficient and reduces the risk of the vanishing gradient problem \cite{prince2023understanding}.
As the preceding operations can increase the dimensionality of the input, a form of down-sampling is applied by using a max-pooling operation which helps in achieving 
translation invariance and reducing computational complexity for the following layers. Something worth considering for 
part 2 of this dissertation is to explore the use of other down-sampling techniques, and in particular,
\citet{liu_simple_2023} have proposed a family of simple pooling front-ends (SimPFs) 
which in some cases reduce the number of FLOPs as much as 75\% in acoustic scene classification tasks. 
Lastly, dropout of 0.3 is applied after the max-pooling operation which can help with 
the model generalisation \cite{prince2023understanding}.

\begin{figure}[h]
   \centering
   \includegraphics[width=0.9\textwidth]{cnn-diagram.png}
   \caption{The baseline model architecture. $B$ is the batch size, and $C$ is the number of classes/environments.
   After each layer we label the output shape of the tensor on the assumption that 
   the net-32 model is used (see Table \ref{tab:cnn-model-params}).
   }
   \label{fig:cnn-model-architecture}
\end{figure}

\subsection{Training}
\subsection{Evaluation}

\section{Speech Enhancement}
\subsection{Model}
\subsection{Training}
\subsection{Evaluation}




\chapter{Experimental Setup}
\section{Datasets}
\label{sec:datasets}
\subsection{HEAR-DS}
\label{sec:hear-ds}
Given how much this dissertation revolves around the HEAR-DS dataset, we dedicate a section solely 
to the dataset. 
HEAR-DS is a dataset created by \citet{Huwel2020HearDS} which is specially tailored for Hearing Aid (HA) research. 
It consists of recordings captured on a dummy head equipped with Pinnae models and three microphones per side: one In-The-Canal (ITC) [in the ear] and two Behind-The-Ear (BTE) microphones (front and rear).
Each sample is recorded in either the left or right channel and decomposed into its respective microphone channels (BTE\_front, BTE\_rear, ITC), available in 48kHz/32bit format.
In the paper, the researchers used only the ITC samples, so for the scope of this project, we will only use the ITC samples as well. 
As the researchers were based in Germany, any recordings that they took were situated in Germany.
The dataset provides 7 environments:
\begin{itemize}
   \item \textbf{Cocktail Party} - Covers situations such as multiple speakers in a noisy environment or 'babble' noise. The paper mentions that this was mimicked by recording them in a university cafeteria or at a senior citizens' meeting.
   \item \textbf{Wind Turbulence} - Concerns the sound that is produced when wind passes through a microphone.
   \item \textbf{In-Traffic} - Samples where traffic noise is dominant such as bus stations or sidewalks. The researchers controlled for this environment to not be similar to the \textit{Wind Turbulence} environment by choosing 'calm' days for the recordings.
   \item \textbf{In-Vehicle} - The sounds produced when seated inside a car. The researchers controlled for this environment to not be similar to the \textit{In-Traffic} or \textit{Wind Turbulence} environments by ensuring the windows were closed. 
   \item \textbf{Quiet Indoors} - Mimic noises produced in a typical household such as washing dishes, clock ticking, etc. The researchers controlled for variability by recording the samples in several flats (rural, city centre, etc).
   \item \textbf{Reveberant} - Samples in highly reverberant environments such as a railway station hall, staircases or a church. 
   \item \textbf{Music} - The data is actually taken from the GTZAN dataset \cite{tzanetakis_musical_2002} and is used to test the model's ability to classify music.
\end{itemize}
Additionally, the researchers artificially created a new environment, \textit{Interfering Speakers}, which are samples that contain speech from multiple speakers. 
This is done by taking speech samples from the CHiME 5 dataset \cite{barker18_fifth_2018}. This environment is supposed to mimic the 
typical conversational speech that occurs in a real-world scenario. We however, decided to use the CHiME 6 dataset which 
supercedes the CHiME 5 dataset \cite{barker18_fifth_2018} by correcting the alignment of audio channels and the researchers 
recommend the use of the CHiME 6 dataset instead of CHiME 5 for any new research so we will use that instead. This could 
be an important factor as the methodology of creating the \textit{Interfering Speakers} environment is by finding 10s 
segments of speech that contain multiple speakers, so if the alignment of audio channels is not correct, the recordings 
may have a delay between the left and right channels which could have potentialy affected the results of the paper. We will discuss the dataset in more detail in section \ref{sec:chime6}.

To prevent the model from overfitting and increase the diversity of the data, the researchers provided multiple REC-SITs (Recording Situations) for each environment.
In the case of the \textit{Music} environment, each REC-SIT represents a different genre of music. This has some potential to cause issues 
and we discuss this in more detail once we aim to reproduce the results of the paper (Chapter \ref{chap:acoustic-scene-classification}). 
As for the artificially created \textit{Interfering Speakers} environment, it is unclear how they define REC-SITs for this environment. 
What I did was to extract the session ID from the CHiME 6 dataset and use that as the REC-SIT, which 
gives us a total of 16 REC-SITs for this environment.

% TOOD: Make sure to refer to this problem later


\subsection{CHiME3}
As mentioned in the previous chapter, the researchers used the CHiME 3 dataset to mix the speech signal with the background noise. 
CHiME 3 is a dataset published in 2015 for ASR tasks, and was dedicated to improving the performance of mobile devices 
in everyday, noisy environments. The vocabulary of the speech samples is taken from a subset of the Wall Street Journal (WSJ0) corupus \cite{TODO}.
We won't go into the details of the environments, because while not exactly clear from 
the paper, it is mentioned that a development set was used for the mixing. We assume that they used the isolated development set 
recorded in a booth, as it would not make sense otherwise since the other sets contain noisy samples.
The use of the development set for mixing is not ideal, as it is not representative of the real-world data.
In particular, as can be seen from Table \ref{tab:chime3-stats}, it contains 10 hours of data from 4 speakers. This could be a problem in both tasks (ASA and Speech Enhancements), in the former
we could be overfitting to the data due to the Pigeon-hole principle i.e. we have much more background samples than speech samples, 
and in the latter, the enhancement model may not generalise well to real-world scenarios due to the small size of the dataset. 
A better approach would of perhaps been to use the cleanm speech samples 
Nevertheless, we will continue with the paper's approach to set up a baseline. The data was already in 16KHz so we did not need to 
resample it. 
% TODO: Add table here
\subsection{CHiME5/CHiME6}
The CHiME 5 dataset was originally used in the paper to create the \textit{Interfering Speakers} environment for the HEAR-DS dataset.
However, since the CHiME 6 dataset supersedes CHiME 5 with improved audio channel alignment, we opted to use CHiME 6 for our implementation.
CHiME 6 (and CHiME 5) was a dataset created for the Speech Seperation and Recognition Challenge in 2020. It 
focuses on conversational speech in everyday home environments and particular emphasis was placed on 
eliciting a 'dinner party' scenario i.e. a mixture of speech from multiple speakers. It was not 
clear from the paper what sets were used for the creation of the \textit{Interfering Speakers} environment 
however, they did provide a table of number of samples in each environment. From the table, we 
were able to deduce that they must have used the training, development and evaluation sets for the creation of the \textit{Interfering Speakers} environment.
We talk more about the creation of this environment when we discuss the baseline model in Chapter \ref{chap:acoustic-scene-classification}.
From Table \ref{tab:chime6-stats}, we can see that the dataset contains almost 50 hours of data from 48 speakers. 
\label{sec:chime6}
TODO Table

\subsection{TUT Acoustic Scenes 2016}
As mentioned in the previous chapter, I came across better results than the baseline model in the paper by \citet{Huwel2020HearDS} 
so as a means of validation, I decided to use the TUT Acoustic Scenes 2016 dataset \cite{mesaros_tut_2016} to test the performance of the model.
In particular, I will comparing the results by \citet{schindler_multi-temporal_2018}. The paper presents a multi-temporal approach to ASA, and 
the authors have used the development set of the TUT Acoustic Scenes 2016 dataset for their experiments. It is worth stressing 
that this dataset contains background noise only, and the paper does not evaluate it with speech, so when we use it to validate 
the model, we will maintain the same approach as the paper.

\subsection{VOICEBANK + DEMAND}
TODO

\section{Hypotheses}

\begin{enumerate}
   \item \textbf{Using Fixed Learning Rate Stochastic Gradient Descent (SGD) will result in a longer training time than using Adam Optimiser.} The paper by \citet{Huwel2020HearDS} used the fixed learning rate SGD approach.
   I hypothesise that using Adam Optimiser will result in a shorter training time.
   \item \textbf{Data Augmentation will result in a higher accuracy of the model.} It is unclear from the paper whether the authors used data augmentation...
   \item \textbf{Using an LSTM model will result in a higher accuracy of the model.}
\end{enumerate}


\chapter{Acoustic Scene Classification Experiments}
\label{chap:acoustic-scene-classification}
\section{Data Preparation}
TODO
\section{Fixed Learning Rate SGD Approach}
TODO
\section{Adam Optimiser Approach}
TODO

\section{Validating against TUT Acoustic Scenes 2016}

\chapter{Speech Enhancement Experiments}
\section{Data Preparation}
TODO
\section{One Model Approach}
TODO
\section{Per Environment Model Approach}
TODO
\section{Validating against VOICEBANK + DEMAND}

\chapter{Conclusions}
\section{Discussion}
TODO
\section{Future Work}
TODO









% \chapter{Conclusions}

% \section{Final Reminder}

% The body of your dissertation, before the references and any appendices,
% \emph{must} finish by page~40. The introduction, after preliminary material,
% should have started on page~1.

% You may not change the dissertation format (e.g., reduce the font size, change
% the margins, or reduce the line spacing from the default single spacing). Be
% careful if you copy-paste packages into your document preamble from elsewhere.
% Some \LaTeX{} packages, such as \texttt{fullpage} or \texttt{savetrees}, change
% the margins of your document. Do not include them!

% Over-length or incorrectly-formatted dissertations will not be accepted and you
% would have to modify your dissertation and resubmit. You cannot assume we will
% check your submission before the final deadline and if it requires resubmission
% after the deadline to conform to the page and style requirements you will be
% subject to the usual late penalties based on your final submission time.

\bibliography{mybibfile}


% You may delete everything from \appendix up to \end{document} if you don't need it.
% \appendix

% \chapter{First appendix}

% \section{First section}

% Any appendices, including any required ethics information, should be included
% after the references.

% Markers do not have to consider appendices. Make sure that your contributions
% are made clear in the main body of the dissertation (within the page limit).

% \chapter{Participants' information sheet}

% If you had human participants, include key information that they were given in
% an appendix, and point to it from the ethics declaration.

% \chapter{Participants' consent form}

% If you had human participants, include information about how consent was
% gathered in an appendix, and point to it from the ethics declaration.
% This information is often a copy of a consent form.


\end{document}